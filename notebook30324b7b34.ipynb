{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3725387,"sourceType":"datasetVersion","datasetId":2227758}],"dockerImageVersionId":30197,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style='color:green'>ABOUT</h1>\n\n<p style='color:lightgreen'>These last decades, Earth Observation brought quantities of new perspectives from geosciences to human activity monitoring. As more data became available, artificial intelligence techniques led to very successful results for understanding remote sensing data. Moreover, various acquisition techniques such as Synthetic Aperture Radar (SAR) can also be used for problems that could not be tackled only through optical images. This is the case for weather-related disasters such as floods or hurricanes, which are generally associated with large clouds cover. Yet, machine learning on SAR data is still considered challenging due to the lack of available labeled data. This dataset is composed of co-registered optical and SAR images time series for the detection of flood events.</p>\n\n[source](https://mlhub.earth/data/sen12floods)  ","metadata":{}},{"cell_type":"markdown","source":"<h2 style='color:green'>Dataset</h2>\n\n<p style='color:lightgreen'>The dataset is composed of 412\ntime series with 4 to 20 optical images and 10 to 58 SAR im-\nages in each sequence. On average, there are 9 optical and 14\nSAR images per sequence. The period of acquisition goes from\nDecember 2018 to May 2019. A flood event is occuring in 40%\nof the optical Sentinel 2 images and in 47% of the SAR Sen-\ntinel 1 images. As in the MediaEval dataset, once a flood oc-\ncurred in a sequence, all the subsequent images are labeled as\nflooded which corresponds to the hypothesis that the surface\nstill presents characteristic modifications after the event.</p>\n\n[source](https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B2-2020/1343/2020/isprs-archives-XLIII-B2-2020-1343-2020.pdf)  ","metadata":{}},{"cell_type":"markdown","source":"<h2 style='color:green'>Other notebooks related to this project</h2>\n\n    \n* [Data Download](https://www.kaggle.com/code/virajkadam/detecting-flood-from-satellite-img-data-download)\n    \n* [The next notebook in this series(Time Series)](https://www.kaggle.com/virajkadam/detecting-floods-time-series-prediction)\n    ","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import json\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport os\nimport gc\nimport rasterio as rio\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib import  cm\nimport cv2\nfrom matplotlib import animation\nfrom IPython.display import HTML\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:17:31.571406Z","iopub.execute_input":"2022-09-26T11:17:31.571947Z","iopub.status.idle":"2022-09-26T11:17:39.185513Z","shell.execute_reply.started":"2022-09-26T11:17:31.571862Z","shell.execute_reply":"2022-09-26T11:17:39.184424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed = 7\n    img_size = (256,256)\n    BATCH_SIZE = 3\n    Autotune = tf.data.AUTOTUNE\n    validation_size = 0.2\n    class_dict= {0:'No Flooding', \n                 1: 'Flooding'}\n    \n    test_run = False \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-09-26T11:17:39.189801Z","iopub.execute_input":"2022-09-26T11:17:39.190946Z","iopub.status.idle":"2022-09-26T11:17:39.199337Z","shell.execute_reply.started":"2022-09-26T11:17:39.1909Z","shell.execute_reply":"2022-09-26T11:17:39.198354Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Input data\n\n    Read more about the dataset here : https://clmrmb.github.io/SEN12-FLOOD/","metadata":{}},{"cell_type":"code","source":"s1_labels = '../input/sen12flood/sen12flood/sen12floods_s1_labels/sen12floods_s1_labels/'\ns1_tiles = '../input/sen12flood/sen12flood/sen12floods_s1_source/sen12floods_s1_source/'\n\ns2_tiles = '../input/sen12flood/sen12flood/sen12floods_s2_source/sen12floods_s2_source/'\ns2_labels = '../input/sen12flood/sen12flood/sen12floods_s2_labels/sen12floods_s2_labels/'\n\n\ns1_check = 0\nfor file in os.listdir(s1_labels):\n    if os.path.exists(s1_tiles + '/' + file.replace('labels','source')):\n        s1_check += 1\n        \n         \nassert s1_check == len(os.listdir(s1_tiles)), 'You my friend , are definintely a idiot!'\n    \ns2_check = 0\nfor file in os.listdir(s2_labels):\n    if os.path.exists(s2_tiles + '/' + file.replace('labels','source')):\n        s2_check += 1\n        \n        \nassert s2_check == len(os.listdir(s2_tiles)), 'You my friend , are definintely  the idiot!'\n\n\ns1_check,s2_check ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:17:39.203772Z","iopub.execute_input":"2022-09-26T11:17:39.20534Z","iopub.status.idle":"2022-09-26T11:17:44.627438Z","shell.execute_reply.started":"2022-09-26T11:17:39.205252Z","shell.execute_reply":"2022-09-26T11:17:44.626527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make a dataset of paths and labels","metadata":{}},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"def load_json(path):\n    '''loads a json file'''\n    with open(path,'r') as file:\n        js = json.load(file)\n        \n    return js\n\n# collectionss1 = load_json('../input/sen12flood/sen12flood/sen12floods_s1_source/sen12floods_s1_source/collection.json')\n# collections2= load_json('../input/sen12flood/sen12flood/sen12floods_s2_source/sen12floods_s2_source/collection.json')\n# collections2","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:17:44.62955Z","iopub.execute_input":"2022-09-26T11:17:44.630098Z","iopub.status.idle":"2022-09-26T11:17:44.637468Z","shell.execute_reply.started":"2022-09-26T11:17:44.630059Z","shell.execute_reply":"2022-09-26T11:17:44.636754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_label_json(label_json):\n    '''process a single label json'''\n    info_dict = {}\n    \n    info_dict['geometry'] = label_json['geometry']['coordinates']\n    info_dict['label'] = label_json['properties']['FLOODING']\n    info_dict['date'] = label_json['properties']['date']\n    info_dict['tile_number'] = label_json['properties']['tile']\n#     info_dict['full_data_coverage']= label_json['properties']['FULL-DATA-COVERAGE']\n    \n    return info_dict\n\n\ndef process_label_stac(stac_json):\n    return stac_json['id']\n    \n    \n\n\ndef image_path_from_label_dir(image_parent_dir:str,\n                              label_file :str)->str:\n    \n    return image_parent_dir + '/' + label_file.replace('labels','source')\n    \n    \n\ndef process_json(label_path,image_directory):\n    '''get the data for a single example\n     Inputs \n     label_path : path to the label folder \n     image_directory: path to the corresponding image directory'''\n    \n    \n\n    #get image directory for that label\n    folder_id = label_path.rsplit('/',1)[1]\n    image_dir_path = image_path_from_label_dir(image_directory,folder_id)\n\n    if not os.path.exists(image_dir_path):\n        return {'File_not_found':image_dir_path}\n    \n    \n    for file in os.listdir(label_path):\n        #if image dir exists \n        if file.startswith('labels'):\n            label_json = load_json(os.path.join(label_path,file))\n        else:\n            stac_json = load_json(os.path.join(label_path,file))\n\n\n    #get data \n    info_dict = process_label_json(label_json)\n\n    #get id \n    info_dict['id'] = process_label_stac(stac_json)\n    \n    #location id \n    info_dict['location_id'] = info_dict['id'].split('_')[3]\n    \n    \n    info_dict['image_dir'] = image_dir_path\n    \n    \n    return info_dict\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:17:44.642764Z","iopub.execute_input":"2022-09-26T11:17:44.643287Z","iopub.status.idle":"2022-09-26T11:17:44.682133Z","shell.execute_reply.started":"2022-09-26T11:17:44.64325Z","shell.execute_reply":"2022-09-26T11:17:44.681405Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dataframe(label_directory,image_directory):\n    '''get dataframe from the nested label directory'''\n    records = []\n    \n        \n    for folder in os.listdir(label_directory):\n        if folder.startswith('sen12'):\n#             print(folder,label_directory)\n            folder_path = label_directory + '/' + folder\n            \n            \n            #get data for a single example\n            feature = process_json(label_path=folder_path,\n                                   image_directory=image_directory)\n            \n            \n            records.append(feature)\n            \n            \n    return pd.DataFrame.from_records(data = records)\n\n\n\ndef type_cast_dataset(dataset):\n    '''typecasting columns in dataset'''\n    dataset['label'] = dataset['label'].astype(int)\n    \n    dataset['date'] = pd.to_datetime(dataset['date'])\n    dataset['tile_number'] = dataset['tile_number'].astype('int8')\n    \n    \n    return dataset\n    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:17:44.683534Z","iopub.execute_input":"2022-09-26T11:17:44.684094Z","iopub.status.idle":"2022-09-26T11:17:44.697387Z","shell.execute_reply.started":"2022-09-26T11:17:44.684058Z","shell.execute_reply":"2022-09-26T11:17:44.696614Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ns1_data = type_cast_dataset(\n                            get_dataframe(\n                                label_directory=s1_labels,\n                                image_directory=s1_tiles\n                                        )\n                            )\n\n\ns2_data = type_cast_dataset(\n                            get_dataframe(label_directory=s2_labels,\n                                          image_directory=s2_tiles)\n                            )\n\nprint(f'Number of unique locations in Sentinel1 (SAR) data : {s1_data.location_id.nunique()}')\nprint(f'Number of unique locations in Sentinel2 (optical) data : {s2_data.location_id.nunique()}')\n\ns1_data.shape,s2_data.shape","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:17:44.698688Z","iopub.execute_input":"2022-09-26T11:17:44.699232Z","iopub.status.idle":"2022-09-26T11:18:26.758297Z","shell.execute_reply.started":"2022-09-26T11:17:44.699198Z","shell.execute_reply":"2022-09-26T11:18:26.757493Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saving datasets\ns1_data.to_csv('s1_data.csv',index=False)\ns2_data.to_csv('s2_data.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T11:18:26.759627Z","iopub.execute_input":"2022-09-26T11:18:26.759984Z","iopub.status.idle":"2022-09-26T11:18:26.855362Z","shell.execute_reply.started":"2022-09-26T11:18:26.759949Z","shell.execute_reply":"2022-09-26T11:18:26.854592Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_raster(filepath):\n    '''load a single band raster'''\n    with rio.open(filepath) as file: \n        raster = file.read().squeeze(axis=0)\n        \n    return raster\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:25.444184Z","iopub.execute_input":"2022-09-26T11:20:25.444543Z","iopub.status.idle":"2022-09-26T11:20:25.448955Z","shell.execute_reply.started":"2022-09-26T11:20:25.444512Z","shell.execute_reply":"2022-09-26T11:20:25.448119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Loading multiple raster bands as single raster**","metadata":{}},{"cell_type":"code","source":"def load_s1_tiffs(folder,\n                  scaling_values=[50.,100.]):\n    images = []\n    i = 0\n    for im in sorted(os.listdir(folder)):\n         \n        if im.rsplit('.',maxsplit=1)[1] == 'tif':\n            \n            path = folder + '/' + im\n            band = load_raster(path)\n            band = band / scaling_values[i]\n            \n            band = cv2.resize(band,\n                              CFG.img_size)\n            \n            images.append(band)\n            i+=1 \n                    \n    return np.dstack(images)\n\n\ndef load_s2_tiffs(folder,\n                  scaling_value=10000.):\n    images = []\n    for im in sorted(os.listdir(folder)):\n        if im.rsplit('.',maxsplit=1)[1] == 'tif':    \n            path = folder + '/' + im\n            band = load_raster(path)\n            band = band/ scaling_value\n            \n            band = cv2.resize(band,CFG.img_size)\n            images.append(band)   \n\n    return np.dstack(images)\n                    \ndef load_rgb_tiffs(folder,\n                  scaling_value=10000.):\n    '''load R,G and B bands'''\n    \n    images = []\n    for im in sorted(os.listdir(folder)):\n        name,file_format = im.rsplit('.',maxsplit=1)\n        if ((file_format== 'tif') and (name in ['B02','B03','B04'])):    \n            path = folder + '/' + im\n            band = load_raster(path)\n            band = band/ scaling_value\n            \n            band = cv2.resize(band,CFG.img_size)\n            images.append(band)   \n\n    return np.dstack(images)[:,:,::-1]\n\n\n    \ndef tf_load_s1(path):    \n    path = path.numpy().decode('utf-8')\n    return load_s1_tiffs(path)\n    \n    \n\ndef tf_load_s2(path):    \n    path = path.numpy().decode('utf-8')\n    return load_s2_tiffs(path)\n\n\ndef tf_load_rgb(path):    \n    path = path.numpy().decode('utf-8')\n    return load_rgb_tiffs(path)\n    \ndef process_image_s1(filename):\n    '''function for preprocessing in tensorflow data'''\n    \n    return tf.py_function(tf_load_s1, \n                          [filename], \n                          tf.float32)\n\n\n\ndef process_image_s2(filename):\n    '''function for preprocessing in tensorflow data'''\n    \n    return tf.py_function(tf_load_s2, \n                          [filename], \n                          tf.float32)\n\n\n\ndef process_image_rgb(filename):\n    '''function for preprocessing in tensorflow data'''\n    \n    return tf.py_function(tf_load_rgb, \n                          [filename], \n                          tf.float32)\n    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:26.789188Z","iopub.execute_input":"2022-09-26T11:20:26.789549Z","iopub.status.idle":"2022-09-26T11:20:26.803006Z","shell.execute_reply.started":"2022-09-26T11:20:26.789519Z","shell.execute_reply":"2022-09-26T11:20:26.80203Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_rasters_in_folder(path):\n    count = 0 \n    \n    for file in os.listdir(path):\n        if file.rsplit('.',1)[1] == 'tif':\n            count +=1 \n            \n    return count \n    \n    \ns2_data['raster_count'] = s2_data.image_dir.apply(lambda x : count_rasters_in_folder(x))\n\n#value counts \ns2_data['raster_count'].value_counts()\n\n\ns2_data=s2_data[s2_data['raster_count']==12] # take only valid rasters\n# s2_data[s2_data['raster_count']==0]['location_id'].value_counts()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:26.920456Z","iopub.execute_input":"2022-09-26T11:20:26.921047Z","iopub.status.idle":"2022-09-26T11:20:28.00142Z","shell.execute_reply.started":"2022-09-26T11:20:26.921018Z","shell.execute_reply":"2022-09-26T11:20:28.000621Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize some images","metadata":{}},{"cell_type":"markdown","source":"**lets take a look at some optical RGB images**","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(16,8))\n\nplt.subplot(1,2,1)\nchk = load_s2_tiffs(s2_data.query('label==1')['image_dir'].values[11])\nplt.imshow(chk[:,:,1:4][:,:,::-1])\nplt.title('Flooding')\nplt.axis('off')\n\n\nchk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[20])\nplt.subplot(1,2,2)\nplt.imshow(chk[:,:,1:4][:,:,::-1])\nplt.title('No Flooding')\nplt.axis('off')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:28.002952Z","iopub.execute_input":"2022-09-26T11:20:28.003605Z","iopub.status.idle":"2022-09-26T11:20:29.04393Z","shell.execute_reply.started":"2022-09-26T11:20:28.003551Z","shell.execute_reply":"2022-09-26T11:20:29.042448Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(16,8))\n\nplt.subplot(1,2,1)\nchk = load_s2_tiffs(s2_data.query('label==1')['image_dir'].values[101])\nplt.imshow(chk[:,:,1:4][:,:,::-1])\nplt.title('Flooding')\nplt.axis('off')\n\n\nchk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[11])\nplt.subplot(1,2,2)\nplt.imshow(chk[:,:,1:4][:,:,::-1])\nplt.title('No Flooding')\nplt.axis('off')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:31.103854Z","iopub.execute_input":"2022-09-26T11:20:31.104243Z","iopub.status.idle":"2022-09-26T11:20:31.78512Z","shell.execute_reply.started":"2022-09-26T11:20:31.10421Z","shell.execute_reply":"2022-09-26T11:20:31.781896Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Some SAR images**","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(16,8))\n\nplt.suptitle('SAR images (VH)')\n\nplt.subplot(1,2,1)\n\n\nchk = load_s1_tiffs(s1_data.query('label==1')['image_dir'].values[11])\nplt.imshow(chk[:,:,1])\nplt.title('Flooding')\nplt.axis('off')\n\n\nchk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[11])\nplt.subplot(1,2,2)\nplt.imshow(chk[:,:,1])\nplt.title('No Flooding')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-09-26T11:20:34.676701Z","iopub.execute_input":"2022-09-26T11:20:34.677069Z","iopub.status.idle":"2022-09-26T11:20:35.314161Z","shell.execute_reply.started":"2022-09-26T11:20:34.677039Z","shell.execute_reply":"2022-09-26T11:20:35.313475Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(16,8))\nplt.suptitle('SAR images (VV)')\n\n\nr_idx = np.random.randint(low=0,high=500)\nplt.subplot(1,2,1)\nchk = load_s1_tiffs(s1_data.query('label==1')['image_dir'].values[r_idx])\nplt.imshow(chk[:,:,0])\nplt.title('Flooding')\nplt.axis('off')\n\n\nchk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[r_idx])\nplt.subplot(1,2,2)\nplt.imshow(chk[:,:,0])\nplt.title('No Flooding')\nplt.axis('off')\nplt.tight_layout()\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:35.315489Z","iopub.execute_input":"2022-09-26T11:20:35.316316Z","iopub.status.idle":"2022-09-26T11:20:35.834857Z","shell.execute_reply.started":"2022-09-26T11:20:35.316281Z","shell.execute_reply":"2022-09-26T11:20:35.834083Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(16,8))\nplt.suptitle('SAR images (VV)')\n\n\nr_idx = np.random.randint(low=0,high=500)\nplt.subplot(1,2,1)\nchk = load_s1_tiffs(s1_data.query('label==1')['image_dir'].values[r_idx])\nplt.imshow(chk[:,:,0])\nplt.title('Flooding')\nplt.axis('off')\n\n\nchk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[r_idx])\nplt.subplot(1,2,2)\nplt.imshow(chk[:,:,0])\nplt.title('No Flooding')\nplt.axis('off')\nplt.tight_layout()\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:35.83656Z","iopub.execute_input":"2022-09-26T11:20:35.837108Z","iopub.status.idle":"2022-09-26T11:20:36.550042Z","shell.execute_reply.started":"2022-09-26T11:20:35.837069Z","shell.execute_reply":"2022-09-26T11:20:36.549348Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Lets look at the distribution of target values**","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(16,8))\n\nplt.subplot(1,2,1)\nsns.countplot(s1_data.label)\nplt.title('Sentinel -1 target distribution')\n\n\nplt.subplot(1,2,2)\nsns.countplot(s2_data.label)\nplt.title('Sentinel - 2 target distribution')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:38.182366Z","iopub.execute_input":"2022-09-26T11:20:38.183059Z","iopub.status.idle":"2022-09-26T11:20:38.445155Z","shell.execute_reply.started":"2022-09-26T11:20:38.183021Z","shell.execute_reply":"2022-09-26T11:20:38.444411Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making a TF dataset","metadata":{}},{"cell_type":"markdown","source":"    First lets split the dataset into training and validation set. We will stratify based on location id to ensure that locations are well represented in traininng and validation set","metadata":{}},{"cell_type":"code","source":"#isolating single loaction ids (as they will be a problem for stratification)\n\n# single example locations \nsingle_index = s2_data['location_id'].value_counts()[s2_data['location_id'].value_counts()==1].index\n\nsingle_index_df = s2_data[s2_data['location_id'].isin(single_index)].reset_index(drop=True)\ns2_data0 = s2_data[~(s2_data['location_id'].isin(single_index))].reset_index(drop=True)\n\ns2_data0.shape,single_index_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-26T11:20:39.134677Z","iopub.execute_input":"2022-09-26T11:20:39.135045Z","iopub.status.idle":"2022-09-26T11:20:39.150722Z","shell.execute_reply.started":"2022-09-26T11:20:39.135013Z","shell.execute_reply":"2022-09-26T11:20:39.149907Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Split dataset into train and validation splits**","metadata":{}},{"cell_type":"code","source":"#traintest split \n\ns1_data_tr,s1_data_val= train_test_split(s1_data,\n                                          test_size = CFG.validation_size,\n                                          random_state = CFG.seed,\n                                          stratify = s1_data.location_id)\n\n\n\ns2_data_tr,s2_data_val = train_test_split(s2_data0,\n                                          test_size = CFG.validation_size,\n                                          random_state = CFG.seed,\n                                          stratify =  s2_data0.location_id)\n\ns2_data_tr = s2_data_tr.append(single_index_df,ignore_index=True)\n\ndel s2_data0;gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:39.957736Z","iopub.execute_input":"2022-09-26T11:20:39.958183Z","iopub.status.idle":"2022-09-26T11:20:40.250023Z","shell.execute_reply.started":"2022-09-26T11:20:39.958142Z","shell.execute_reply":"2022-09-26T11:20:40.247659Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s1_data_tr.label.value_counts(1),s1_data_val.label.value_counts(1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:41.042192Z","iopub.execute_input":"2022-09-26T11:20:41.042803Z","iopub.status.idle":"2022-09-26T11:20:41.05281Z","shell.execute_reply.started":"2022-09-26T11:20:41.042766Z","shell.execute_reply":"2022-09-26T11:20:41.051772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s2_data_tr.label.value_counts(1),s2_data_val.label.value_counts(1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:41.184185Z","iopub.execute_input":"2022-09-26T11:20:41.184518Z","iopub.status.idle":"2022-09-26T11:20:41.194118Z","shell.execute_reply.started":"2022-09-26T11:20:41.184489Z","shell.execute_reply":"2022-09-26T11:20:41.19315Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Function for image augmentations**\n\n    Although the Augmentations are simple, we cannot use them on SAR images , as even simple operations like flipping can change the meaning of the image","metadata":{}},{"cell_type":"code","source":"def augment_image_multispectral(image):\n    '''perform simple image augmentations'''\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_crop(image, size=(*CFG.img_size,12))\n    \n    rot = tf.random.normal((1,),mean = 0.35, stddev=0.15)\n    \n    if rot > 0.5:\n        image = tf.image.rot90(image)\n\n    return image \n\ndef augment_image(image):\n    '''perform simple image augmentations'''\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_crop(image, size=(*CFG.img_size,3))\n    \n    rot = tf.random.normal((1,),mean = 0.35, stddev=0.15)\n    \n    if rot > 0.5:\n        image = tf.image.rot90(image)\n\n    return image ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:49.333889Z","iopub.execute_input":"2022-09-26T11:20:49.334259Z","iopub.status.idle":"2022-09-26T11:20:49.341871Z","shell.execute_reply.started":"2022-09-26T11:20:49.334227Z","shell.execute_reply":"2022-09-26T11:20:49.340804Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_tf_dataset(image_paths,\n                   labels=None, # put none for test data set\n                   image_processing_fn=None,\n                   augment_fn = None\n                  ):\n    \n    '''returns a tf dataset object\n    Inputs: \n    image_paths : paths to images\n    labels: labels of each image\n    image_processing_fn:  function to load and preprocess images \n    augment_fn : function to augment images '''\n    \n    #seperate datasets\n    if labels is not None:\n        labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n    \n    \n    \n    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n    #load images \n    image_dataset = image_dataset.map(image_processing_fn,\n                                      num_parallel_calls=tf.data.AUTOTUNE)\n     \n    if augment_fn is not None:\n        \n        image_dataset = image_dataset.map(augment_fn,\n                                          num_parallel_calls=tf.data.AUTOTUNE)\n     \n    \n    if labels is not None:\n        return tf.data.Dataset.zip((image_dataset,labels_dataset))\n    \n    \n    return image_dataset\n\n\n\ndef optimize_pipeline(tf_dataset,\n                      batch_size = CFG.BATCH_SIZE,\n                      Autotune_fn = CFG.Autotune,\n                      cache= False,\n                      batch = True):\n    \n    \n    \n    # prefetch(load the data with cpu,while gpu is training) the data in memory \n    tf_dataset = tf_dataset.prefetch(buffer_size=Autotune_fn)  \n    if cache:\n        tf_dataset = tf_dataset.cache()                        # store data in RAM  \n        \n    tf_dataset =  tf_dataset.shuffle(buffer_size=50)         # shuffle \n    \n    if batch:\n        tf_dataset = tf_dataset.batch(batch_size)              #split the data in batches  \n    \n    return tf_dataset","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:50.675989Z","iopub.execute_input":"2022-09-26T11:20:50.67682Z","iopub.status.idle":"2022-09-26T11:20:50.685098Z","shell.execute_reply.started":"2022-09-26T11:20:50.676786Z","shell.execute_reply":"2022-09-26T11:20:50.68435Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Making dataset pipelines with TF data**","metadata":{}},{"cell_type":"code","source":"# Sentinel 1 dataset (not using augmentation here)\n\nS1_dataset_tr = optimize_pipeline(tf_dataset=get_tf_dataset(image_paths = s1_data_tr.image_dir.values,\n                                               labels = s1_data_tr.label,\n                                               image_processing_fn = process_image_s1),\n                                  \n                                  batch_size = 3 * CFG.BATCH_SIZE)\n\n\nS1_dataset_val = optimize_pipeline(tf_dataset = get_tf_dataset(image_paths = s1_data_val.image_dir.values,\n                                                           labels = s1_data_val.label,\n                                                           image_processing_fn = process_image_s1 ),\n                                   batch_size = 3* CFG.BATCH_SIZE\n                                  )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:51.709776Z","iopub.execute_input":"2022-09-26T11:20:51.710133Z","iopub.status.idle":"2022-09-26T11:20:54.658028Z","shell.execute_reply.started":"2022-09-26T11:20:51.710102Z","shell.execute_reply":"2022-09-26T11:20:54.657145Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#sentinel 2 dataset \nS2_dataset_tr = optimize_pipeline(get_tf_dataset(image_paths = s2_data_tr.image_dir.values,\n                                                   labels = s2_data_tr.label,\n                                                   image_processing_fn = process_image_s2,\n                                                   augment_fn = augment_image_multispectral)\n                                 )\n\n\nS2_dataset_val = optimize_pipeline(get_tf_dataset(image_paths = s2_data_val.image_dir.values,\n                                                   labels = s2_data_val.label,\n                                                   image_processing_fn = process_image_s2,\n                                                   augment_fn = augment_image_multispectral)\n                                  )\n\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:55.827872Z","iopub.execute_input":"2022-09-26T11:20:55.828238Z","iopub.status.idle":"2022-09-26T11:20:56.203608Z","shell.execute_reply.started":"2022-09-26T11:20:55.828207Z","shell.execute_reply":"2022-09-26T11:20:56.202775Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RGB_dataset_tr = optimize_pipeline(get_tf_dataset(image_paths = s2_data_tr.image_dir.values,\n                                                   labels = s2_data_tr.label,\n                                                   image_processing_fn = process_image_rgb,\n                                                   augment_fn = augment_image),\n                                   batch_size = 3* CFG.BATCH_SIZE\n                                 )\n\n\nRGB_dataset_val = optimize_pipeline(get_tf_dataset(image_paths = s2_data_val.image_dir.values,\n                                                   labels = s2_data_val.label,\n                                                   image_processing_fn = process_image_rgb,\n                                                   augment_fn = augment_image),\n                                    batch_size = 3* CFG.BATCH_SIZE\n                                  )\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:20:56.586846Z","iopub.execute_input":"2022-09-26T11:20:56.587354Z","iopub.status.idle":"2022-09-26T11:20:56.957337Z","shell.execute_reply.started":"2022-09-26T11:20:56.587319Z","shell.execute_reply":"2022-09-26T11:20:56.956546Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Checking the values of pixels in the dataset**","metadata":{}},{"cell_type":"code","source":"\n# max_vals_vv = []\n# mean_vals_vv =[]\n\n\n# max_vals_vh = []\n# mean_vals_vh =[]\n\n# for x,y in S1_dataset_tr.as_numpy_iterator():\n#     # vv band \n#     max_vals_vv.append(x[:,:,:,0].max()); mean_vals_vv.append(x[:,:,:,0].mean())\n    \n#     # vh band \n#     max_vals_vh.append(x[:,:,:,1].max()); mean_vals_vh.append(x[:,:,:,1].mean())\n    \n# # band 1value distributions \n# plt.figure(figsize=(16,10))\n\n# sns.distplot(max_vals_vv,label = 'VV band Max values',color='b')\n# # sns.distplot(mean_vals_vv,label = 'VV band Mean values',color = 'g')\n\n# plt.legend()\n\n# plt.title('VV band values Distribution')\n# plt.show()\n\n# # band 1value distributions \n# plt.figure(figsize=(16,10))\n\n# sns.distplot(max_vals_vh,label = 'VH band Max values',color='b')\n# # sns.distplot(mean_vals_vh,label = 'VH band Mean values',color = 'g')\n\n\n# plt.title('VH band values Distribution')\n\n# plt.legend()\n# plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:18:35.414711Z","iopub.status.idle":"2022-09-26T11:18:35.415401Z","shell.execute_reply.started":"2022-09-26T11:18:35.415172Z","shell.execute_reply":"2022-09-26T11:18:35.415197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Checking to see if the Pipelines work as expected**","metadata":{}},{"cell_type":"code","source":"for x,y in S1_dataset_tr.take(1): # take one batch for checking \n    print(f'shape of SAR dataset input {x.shape}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:21:03.786345Z","iopub.execute_input":"2022-09-26T11:21:03.786771Z","iopub.status.idle":"2022-09-26T11:21:05.336129Z","shell.execute_reply.started":"2022-09-26T11:21:03.786734Z","shell.execute_reply":"2022-09-26T11:21:05.335191Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for x,y in S2_dataset_tr.take(1): # take one batch for checking \n    print(f'shape of MultiSpectral dataset input {x.shape}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:21:05.337856Z","iopub.execute_input":"2022-09-26T11:21:05.338245Z","iopub.status.idle":"2022-09-26T11:21:09.84121Z","shell.execute_reply.started":"2022-09-26T11:21:05.338204Z","shell.execute_reply":"2022-09-26T11:21:09.839157Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for x,y in RGB_dataset_tr.take(1): # take one batch for checking \n    print(f'shape of MultiSpectral dataset input {x.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-09-26T11:21:09.844502Z","iopub.execute_input":"2022-09-26T11:21:09.844893Z","iopub.status.idle":"2022-09-26T11:21:11.083431Z","shell.execute_reply.started":"2022-09-26T11:21:09.844863Z","shell.execute_reply":"2022-09-26T11:21:11.082676Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  CNN Models\n    CNN models to identify flooding in opotical and SAR images","metadata":{}},{"cell_type":"code","source":"def multichannel_cnn(num_channels:int,\n                     hidden_units:int, #number of  hidden dense \n                     weights = None  # none for random init, use imagenet for imagenet weights \n                    ):\n    '''model that takes multiple channel as input, instead of using the rgb channels as by default'''\n    \n    \n    backbone = tf.keras.applications.resnet_v2.ResNet50V2(\n                                            include_top=False,\n                                            input_shape = (*CFG.img_size,num_channels),\n                                            weights=weights,\n                                            pooling = 'avg')\n    \n    \n    x = tf.keras.layers.BatchNormalization()(backbone.output)\n    x = tf.keras.layers.Dense(hidden_units,\n                              activation = 'relu')(x)\n    \n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(rate = 0.2)(x)\n\n    \n    \n    \n    final_out = tf.keras.layers.Dense(2,\n                                      activation = 'softmax')(x)\n    \n    \n    #make a model \n    model = tf.keras.Model(inputs = backbone.input, \n                  outputs = final_out)\n    \n    return model \n\n\n# plot train and val acc as  a function of epochs\ndef plot_history(history,addn_metric=None):\n    '''\n    Inputs\n    history:history object from tensorflow\n    add_metric: metric name in the history (like f1_score)'''\n    his=pd.DataFrame(history.history)\n    \n    if addn_metric:\n        plt.subplots(1,3,figsize=(20,6))\n        \n        #loss:\n        ax1=plt.subplot(1,3,1)\n        ax1.plot(range(len(his)),his['loss'],color='g',label='training')\n        ax1.plot(range(len(his)),his['val_loss'],color='r',label='validation')\n        ax1.set_xlabel('EPOCHS')\n        ax1.set_ylabel('LOSS')\n        ax1.legend()\n        ax1.set_title('Loss Per Epoch')\n\n        #accuracy\n        ax2=plt.subplot(1,3,2)\n        ax2.plot(range(len(his)),his['accuracy'],color='g',label='training_acc')\n        ax2.plot(range(len(his)),his['val_accuracy'],color='r',label='validation_acc')\n        ax2.set_xlabel('EPOCHS')\n        ax2.set_ylabel('Accuracy')\n        ax2.legend()\n        ax2.set_title('Accuracy Per Epoch')\n\n    \n        \n        ax3= plt.subplot(1,3,3)\n        ax3.plot(range(len(his)),his[f'{addn_metric}'],color='g',label='training')\n        ax3.plot(range(len(his)),his[f'val_{addn_metric}'],color='r',label='validation')\n        ax3.set_xlabel('EPOCHS')\n        ax3.set_ylabel(f'{addn_metric}')\n        ax3.legend()\n        ax3.set_title(f'{addn_metric} Per Epoch')\n\n        \n    else:\n        plt.subplots(1,2,figsize=(20,8))\n        \n    \n    \n        #loss:\n        ax1=plt.subplot(1,2,1)\n        ax1.plot(range(len(his)),his['loss'],color='g',label='training')\n        ax1.plot(range(len(his)),his['val_loss'],color='r',label='validation')\n        ax1.set_xlabel('EPOCHS')\n        ax1.set_ylabel('LOSS')\n        ax1.legend()\n        ax1.set_title('Loss Per Epoch')\n\n        #accuracy\n        ax2=plt.subplot(1,2,2)\n        ax2.plot(range(len(his)),his['accuracy'],color='g',label='training_acc')\n        ax2.plot(range(len(his)),his['val_accuracy'],color='r',label='validation_acc')\n        ax2.set_xlabel('EPOCHS')\n        ax2.set_ylabel('Accuracy')\n        ax2.legend()\n        ax2.set_title('Accuracy Per Epoch')\n\n        \n    \n    \n    plt.show()  ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:21:17.584995Z","iopub.execute_input":"2022-09-26T11:21:17.585362Z","iopub.status.idle":"2022-09-26T11:21:17.604232Z","shell.execute_reply.started":"2022-09-26T11:21:17.585328Z","shell.execute_reply":"2022-09-26T11:21:17.602103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics ","metadata":{}},{"cell_type":"code","source":"\n\n#from https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_score(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    \n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:21:19.206112Z","iopub.execute_input":"2022-09-26T11:21:19.20648Z","iopub.status.idle":"2022-09-26T11:21:19.213861Z","shell.execute_reply.started":"2022-09-26T11:21:19.206449Z","shell.execute_reply":"2022-09-26T11:21:19.212908Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building and training the SAR CNN","metadata":{}},{"cell_type":"code","source":"SAR_CNN = multichannel_cnn(num_channels = 2,\n                           hidden_units = 512, #number of  hidden dense \n                          )\n\n\n\nSAR_CNN.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n                loss = 'sparse_categorical_crossentropy',\n                metrics = ['accuracy',f1_score,recall_m,precision_m]\n               )\n\n#check on some data \n# SAR_CNN(x)\n\n\n\n!mkdir CNN_models","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:21:22.263209Z","iopub.execute_input":"2022-09-26T11:21:22.263567Z","iopub.status.idle":"2022-09-26T11:21:24.105606Z","shell.execute_reply.started":"2022-09-26T11:21:22.263536Z","shell.execute_reply":"2022-09-26T11:21:24.104257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining callbacks","metadata":{}},{"cell_type":"code","source":"EPOCHS = 2 if CFG.test_run else 75\n# callbacks \n#reduce_learning rate\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(patience=3,\n                                                factor=0.75,\n                                                min_delta=1e-2,\n                                                monitor='val_accuracy',\n                                                verbose=1,\n                                                mode='max')\n\n#early stopping \nearly_stopping=tf.keras.callbacks.EarlyStopping(patience=15,\n                                              min_delta=1e-3,\n                                              monitor='val_accuracy',\n                                              restore_best_weights=True,\n                                              mode='max')\n\n\n# exponential decay \n\ndef lr_scheduler(epoch, lr):\n    '''learning rate scheduler, decays expo after the tenth epoch'''\n\n    if epoch < 10:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n    \n\n    \nlearning_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n\n\ncallbacks_1= [reduce_lr,early_stopping,learning_scheduler]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:21:25.560504Z","iopub.execute_input":"2022-09-26T11:21:25.560948Z","iopub.status.idle":"2022-09-26T11:21:25.570248Z","shell.execute_reply.started":"2022-09-26T11:21:25.560912Z","shell.execute_reply":"2022-09-26T11:21:25.568987Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training on SAR data**","metadata":{}},{"cell_type":"code","source":"hist1 = SAR_CNN.fit(S1_dataset_tr,\n                    validation_data = S1_dataset_val,\n                    epochs = EPOCHS,\n                    callbacks = callbacks_1\n                   )\n\n\n#save model\nsar_model_path = 'CNN_models/SAR_CNN.h5'\nSAR_CNN.save(filepath = 'CNN_models/SAR_CNN.h5')\n\n\n#plot history \nplot_history(hist1,'f1_score')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:21:33.584743Z","iopub.execute_input":"2022-09-26T11:21:33.585117Z","iopub.status.idle":"2022-09-26T11:55:12.4072Z","shell.execute_reply.started":"2022-09-26T11:21:33.585085Z","shell.execute_reply":"2022-09-26T11:55:12.406424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****Evaluate on validation dataset****","metadata":{}},{"cell_type":"code","source":"SAR_CNN.evaluate(S1_dataset_val)\n\ndel SAR_CNN;gc.collect()\n\nK.clear_session()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T12:02:01.483411Z","iopub.execute_input":"2022-09-26T12:02:01.484252Z","iopub.status.idle":"2022-09-26T12:02:08.769167Z","shell.execute_reply.started":"2022-09-26T12:02:01.484213Z","shell.execute_reply":"2022-09-26T12:02:08.768221Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model explainablity ","metadata":{}},{"cell_type":"markdown","source":"    Saliency and Grad Cam. Lets look at what the model is looking at while making htat decision","metadata":{}},{"cell_type":"code","source":"!pip install tf_keras_vis -q\nimport tf_keras_vis \n\nfrom tf_keras_vis.saliency import Saliency\nfrom tf_keras_vis.gradcam import Gradcam\nfrom tf_keras_vis.utils.scores import CategoricalScore\nfrom tf_keras_vis.utils.model_modifiers import ReplaceToLinear\nfrom tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n\n\ntf_keras_vis.__version__\n\n\ndef model_modifier_function(cloned_model):\n    '''modify model activation'''\n    cloned_model.layers[-1].activation = tf.keras.activations.linear\n\n    \ndef get_saliency(img,\n                 score,\n                 cnn_model,\n                 model_modifier=model_modifier_function):\n    #saliency map\n    \n\n    # Create Saliency object.\n    saliency = Saliency(cnn_model,\n                        model_modifier=model_modifier_function,\n                        clone=True)\n    #saliency map \n    sal_map  = saliency(score,\n                        img,\n                        smooth_samples=20, # The number of calculating gradients iterations.\n                        smooth_noise=0.20) # noise spread level.\n    return sal_map\n\ndef get_gradcam(img,\n                score,\n                cnn_model,\n                model_modifier=model_modifier_function):\n\n    # Create Gradcam object\n    gradcam = Gradcam(cnn_model,\n                      model_modifier,\n                      clone=True)\n\n    # Generate heatmap with GradCAM\n    cam = gradcam(score,\n                  img,\n                  seek_penultimate_conv_layer=True)\n    \n    heatmap = np.uint8(cm.jet(cam[0])[..., :3] * 255)\n    \n    \n    return heatmap\n\ndef get_gradcam_plus(img,\n                    score,\n                    model,\n                    model_modifier=ReplaceToLinear()):\n    \n    # Create GradCAM++ object\n    gradcam = GradcamPlusPlus(model,\n                          model_modifier=model_modifier,\n                          clone=True)\n    \n    cam = gradcam(score,\n                  img)\n    \n    heatmap = np.uint8(cm.jet(cam[0])[..., :3] * 255)\n    \n    return heatmap","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T12:02:15.312955Z","iopub.execute_input":"2022-09-26T12:02:15.313825Z","iopub.status.idle":"2022-09-26T12:02:27.266161Z","shell.execute_reply.started":"2022-09-26T12:02:15.313788Z","shell.execute_reply":"2022-09-26T12:02:27.265201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SAR_CNN = tf.keras.models.load_model('CNN_models/SAR_CNN.h5',\n                                     custom_objects={'f1_score':f1_score,\n                                                     'recall_m':recall_m,\n                                                     'precision_m':precision_m}\n                                    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T12:02:27.268354Z","iopub.execute_input":"2022-09-26T12:02:27.268686Z","iopub.status.idle":"2022-09-26T12:02:28.502804Z","shell.execute_reply.started":"2022-09-26T12:02:27.268648Z","shell.execute_reply":"2022-09-26T12:02:28.501629Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nplt.subplots(4,4,figsize=(8*3,8*3))\nn = 4\nidx= 1\n\n\nfor images,labels in S1_dataset_val.shuffle(buffer_size=5).take(1):\n    print(images.shape,labels.shape)\n\n    for i in range(4):\n        #get label \n        img = images[i]\n        lab = int(labels[i].numpy())\n        \n        #predict on image\n        pred = np.argmax(SAR_CNN.predict(img[tf.newaxis,:,:,:]))\n        prd = int(pred.ravel())\n\n        score1 = CategoricalScore(lab)\n\n        plt.subplot(4,4,idx)\n        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n        plt.axis('off')\n        plt.imshow(img[:,:,0])\n        idx+=1\n\n        #saliency\n\n        plt.subplot(4,4,idx)\n        plt.title(f'predicted {CFG.class_dict[prd]} (saliency map)')\n        sal = get_saliency(img,\n                           score1,\n                           cnn_model = SAR_CNN).squeeze(axis=0)\n        \n#         print(sal.shape)\n        plt.axis('off')\n        plt.imshow(img[:,:,0])\n        plt.imshow(sal,alpha=0.25,cmap='jet') #overlay\n        idx+=1\n\n        #gradcam\n        plt.subplot(4,4,idx)\n        gdcam = get_gradcam(img,\n                            score1,\n                           cnn_model = SAR_CNN)\n        plt.imshow(img[:,:,0])\n        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n        plt.axis('off')\n        idx+=1\n\n\n        #gradcam ++\n        plt.subplot(4,4,idx)\n        gdcam_pls = get_gradcam_plus(img,\n                                     score1,\n                                     model = SAR_CNN)\n        plt.imshow(img[:,:,0])\n        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n        plt.axis('off')\n        idx+=1\n\n        if idx>16:\n            break\n\n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T12:02:29.560739Z","iopub.execute_input":"2022-09-26T12:02:29.561223Z","iopub.status.idle":"2022-09-26T12:02:58.372441Z","shell.execute_reply.started":"2022-09-26T12:02:29.56119Z","shell.execute_reply":"2022-09-26T12:02:58.371448Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.subplots(4,4,figsize=(8*3,8*3))\nn = 4\nidx= 1\n\nfor images,labels in S1_dataset_val.shuffle(buffer_size=5).take(1):\n    print(images.shape,labels.shape)\n\n    for i in range(4):\n        #get label \n        img = images[i]\n        lab = int(labels[i].numpy())\n     \n        #predict on image\n        pred = np.argmax(SAR_CNN.predict(img[tf.newaxis,:,:,:]))\n        score1 = CategoricalScore(lab)\n        prd = int(pred.ravel())\n\n        plt.subplot(4,4,idx)\n        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n        plt.axis('off')\n        plt.imshow(img[:,:,1])\n        idx+=1\n        #saliency\n        plt.subplot(4,4,idx)\n        plt.title(f'predicted {CFG.class_dict[prd]} (saliency map)')\n        sal = get_saliency(img,\n                           score1,\n                           cnn_model = SAR_CNN).squeeze(axis=0)\n        \n        plt.axis('off')\n        plt.imshow(img[:,:,1])\n        plt.imshow(sal,alpha=0.25,cmap='jet') #overlay\n        idx+=1\n\n        #gradcam\n        plt.subplot(4,4,idx)\n        gdcam = get_gradcam(img,\n                            score1,\n                           cnn_model = SAR_CNN)\n        plt.imshow(img[:,:,1])\n        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n        plt.axis('off')\n        idx+=1\n\n\n        #gradcam ++\n        plt.subplot(4,4,idx)\n        gdcam_pls = get_gradcam_plus(img,\n                                     score1,\n                                     model = SAR_CNN)\n        plt.imshow(img[:,:,1])\n        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n        plt.axis('off')\n        idx+=1\n\n        if idx>16:\n            break\n\n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T12:02:58.374269Z","iopub.execute_input":"2022-09-26T12:02:58.374717Z","iopub.status.idle":"2022-09-26T12:03:26.310651Z","shell.execute_reply.started":"2022-09-26T12:02:58.374681Z","shell.execute_reply":"2022-09-26T12:03:26.309648Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#delete \ndel SAR_CNN;S1_dataset_val;S1_dataset_tr;gc.collect()\n\nK.clear_session()","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:03:26.313193Z","iopub.execute_input":"2022-09-26T12:03:26.313767Z","iopub.status.idle":"2022-09-26T12:03:26.680021Z","shell.execute_reply.started":"2022-09-26T12:03:26.31373Z","shell.execute_reply":"2022-09-26T12:03:26.679224Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building and training the RGB - CNN","metadata":{}},{"cell_type":"code","source":"RGB_CNN = multichannel_cnn(num_channels = 3,\n                           hidden_units = 512, #number of  hidden dense\n                           weights = 'imagenet'\n                          )\n\n\nRGB_CNN.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n                loss = 'sparse_categorical_crossentropy',\n                metrics = ['accuracy',f1_score,recall_m,precision_m])","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:06:47.115283Z","iopub.execute_input":"2022-09-26T12:06:47.115896Z","iopub.status.idle":"2022-09-26T12:06:48.907061Z","shell.execute_reply.started":"2022-09-26T12:06:47.115851Z","shell.execute_reply":"2022-09-26T12:06:48.906234Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hist2 = RGB_CNN.fit(RGB_dataset_tr,\n                    validation_data = RGB_dataset_val,\n                    epochs = EPOCHS,\n                    callbacks = callbacks_1)\n\n#save model\n\nRGB_CNN.save(filepath = 'CNN_models/RGB_CNN.h5')\n\nplot_history(hist2,'f1_score')","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:06:51.782268Z","iopub.execute_input":"2022-09-26T12:06:51.782747Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Checking the GradCAM and saliency Maps for RGB CNN**","metadata":{}},{"cell_type":"code","source":"\n\nplt.subplots(4,4,figsize=(8*3,8*3))\nn = 4\nidx= 1\n\n\nfor images,labels in RGB_dataset_val.shuffle(buffer_size=12).take(1):\n\n    for i in range(4):\n        #get label \n        img = images[i]\n        lab = int(labels[i].numpy())\n\n\n#         print(img.shape,lab.shape)\n        score1 = CategoricalScore(lab)\n\n\n\n        #predict on image\n        prd= np.argmax(RGB_CNN.predict(img[tf.newaxis,:,:,:]))\n\n\n        plt.subplot(4,4,idx)\n        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n        plt.axis('off')\n        plt.imshow(img)\n        idx+=1\n\n        #saliency\n\n        plt.subplot(4,4,idx)\n        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n        sal = get_saliency(img,\n                           score1,\n                           cnn_model = RGB_CNN).squeeze(axis=0)\n        \n#         print(sal.shape)\n        plt.axis('off')\n        plt.imshow(img)\n        plt.imshow(sal,alpha=0.45,cmap='jet') #overlay\n        idx+=1\n\n        #gradcam\n        plt.subplot(4,4,idx)\n        gdcam = get_gradcam(img,\n                            score1,\n                           cnn_model = RGB_CNN)\n        plt.imshow(img)\n        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n        plt.axis('off')\n        idx+=1\n\n\n        #gradcam ++\n        plt.subplot(4,4,idx)\n        gdcam_pls = get_gradcam_plus(img,\n                                     score1,\n                                     model = RGB_CNN)\n        plt.imshow(img)\n        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n        plt.axis('off')\n        idx+=1\n\n        if idx>16:\n            break\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:18:35.447909Z","iopub.status.idle":"2022-09-26T11:18:35.44864Z","shell.execute_reply.started":"2022-09-26T11:18:35.448349Z","shell.execute_reply":"2022-09-26T11:18:35.448374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.subplots(4,4,figsize=(8*3,8*3))\nn = 4\nidx= 1\n\n\nfor images,labels in RGB_dataset_val.shuffle(buffer_size=12).take(1):\n\n    for i in range(4):\n        #get label \n        img = images[i]\n        lab = int(labels[i].numpy())\n\n\n#         print(img.shape,lab.shape)\n        score1 = CategoricalScore(lab)\n\n\n\n        #predict on image\n        prd= np.argmax(RGB_CNN.predict(img[tf.newaxis,:,:,:]))\n\n\n        plt.subplot(4,4,idx)\n        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n        plt.axis('off')\n        plt.imshow(img)\n        idx+=1\n\n        #saliency\n\n        plt.subplot(4,4,idx)\n        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n        sal = get_saliency(img,\n                           score1,\n                           cnn_model = RGB_CNN).squeeze(axis=0)\n        \n#         print(sal.shape)\n        plt.axis('off')\n        plt.imshow(img)\n        plt.imshow(sal,alpha=0.45,cmap='jet') #overlay\n        idx+=1\n\n        #gradcam\n        plt.subplot(4,4,idx)\n        gdcam = get_gradcam(img,\n                            score1,\n                           cnn_model = RGB_CNN)\n        plt.imshow(img)\n        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n        plt.axis('off')\n        idx+=1\n\n\n        #gradcam ++\n        plt.subplot(4,4,idx)\n        gdcam_pls = get_gradcam_plus(img,\n                                     score1,\n                                     model = RGB_CNN)\n        plt.imshow(img)\n        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n        plt.axis('off')\n        idx+=1\n\n        if idx>16:\n            break\n\n    plt.tight_layout()\n    plt.show()\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:18:35.449816Z","iopub.status.idle":"2022-09-26T11:18:35.45048Z","shell.execute_reply.started":"2022-09-26T11:18:35.450252Z","shell.execute_reply":"2022-09-26T11:18:35.450276Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#delete \ndel RGB_CNN;gc.collect()\n\nK.clear_session()","metadata":{"execution":{"iopub.status.busy":"2022-09-26T11:18:35.451633Z","iopub.status.idle":"2022-09-26T11:18:35.452291Z","shell.execute_reply.started":"2022-09-26T11:18:35.452051Z","shell.execute_reply":"2022-09-26T11:18:35.452074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building and training the Multispectral - CNN","metadata":{}},{"cell_type":"code","source":"multispectral_CNN = multichannel_cnn(num_channels = 12,\n                                    hidden_units = 512, #number of  hidden dense \n                                    )\n\n\n#check on some data \n# multispectral_CNN(x)\n\n\nmultispectral_CNN.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001,momentum = 0.0),\n                loss = 'sparse_categorical_crossentropy',\n                metrics = ['accuracy',f1_score,recall_m,precision_m])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:18:35.453538Z","iopub.status.idle":"2022-09-26T11:18:35.454256Z","shell.execute_reply.started":"2022-09-26T11:18:35.454002Z","shell.execute_reply":"2022-09-26T11:18:35.454027Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nhist2 = multispectral_CNN.fit(S2_dataset_tr,\n                    validation_data = S2_dataset_val,\n                    epochs = EPOCHS,\n                    callbacks = callbacks_1)\n\n#save model\n\nmultispectral_CNN.save(filepath = 'CNN_models/S2_CNN.h5')\n\nplot_history(hist2,'f1_score')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-09-26T11:18:35.455455Z","iopub.status.idle":"2022-09-26T11:18:35.456181Z","shell.execute_reply.started":"2022-09-26T11:18:35.455927Z","shell.execute_reply":"2022-09-26T11:18:35.455953Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Checking the decision mechanism for Multispectral model**","metadata":{}},{"cell_type":"code","source":"\n\nplt.subplots(4,4,figsize=(8*3,8*3))\nn = 4\nidx= 1\n\n\nfor images,labels in S2_dataset_val.shuffle(buffer_size=12).take(1):\n    print(images.shape,labels.shape)\n\n    for i in range(4):\n        #get label \n        img = images[i]\n        lab = int(labels[i].numpy())\n\n\n#         print(img.shape,lab.shape)\n        score1 = CategoricalScore(lab)\n\n\n\n        #predict on image\n        prd= np.argmax(multispectral_CNN.predict(img[tf.newaxis,:,:,:]))\n\n\n        plt.subplot(4,4,idx)\n        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n        plt.axis('off')\n        plt.imshow(img[:,:,1:4][:,:,::-1])\n        idx+=1\n\n        #saliency\n\n        plt.subplot(4,4,idx)\n        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n        sal = get_saliency(img,\n                           score1,\n                           cnn_model = multispectral_CNN).squeeze(axis=0)\n        \n#         print(sal.shape)\n        plt.axis('off')\n        plt.imshow(img[:,:,1:4][:,:,::-1])\n        plt.imshow(sal,alpha=0.45,cmap='jet') #overlay\n        idx+=1\n\n        #gradcam\n        plt.subplot(4,4,idx)\n        gdcam = get_gradcam(img,\n                            score1,\n                           cnn_model = multispectral_CNN)\n        plt.imshow(img[:,:,1:4][:,:,::-1])\n        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n        plt.axis('off')\n        idx+=1\n\n\n        #gradcam ++\n        plt.subplot(4,4,idx)\n        gdcam_pls = get_gradcam_plus(img,\n                                     score1,\n                                     model = multispectral_CNN)\n        plt.imshow(img[:,:,1:4][:,:,::-1])\n        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n        plt.axis('off')\n        idx+=1\n\n        if idx>16:\n            break\n\n    plt.tight_layout()\n    plt.show()\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:18:35.457484Z","iopub.status.idle":"2022-09-26T11:18:35.458198Z","shell.execute_reply.started":"2022-09-26T11:18:35.457945Z","shell.execute_reply":"2022-09-26T11:18:35.457971Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nplt.subplots(4,4,figsize=(8*3,8*3))\nn = 4\nidx= 1\n\n\nfor images,labels in S2_dataset_val.shuffle(buffer_size=12).take(1):\n    print(images.shape,labels.shape)\n\n    for i in range(4):\n        #get label \n        img = images[i]\n        lab = int(labels[i].numpy())\n\n\n#         print(img.shape,lab.shape)\n        score1 = CategoricalScore(lab)\n\n\n\n        #predict on image\n        prd= np.argmax(multispectral_CNN.predict(img[tf.newaxis,:,:,:]))\n\n\n        plt.subplot(4,4,idx)\n        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n        plt.axis('off')\n        plt.imshow(img[:,:,1:4][:,:,::-1])\n        idx+=1\n\n        #saliency\n\n        plt.subplot(4,4,idx)\n        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n        sal = get_saliency(img,\n                           score1,\n                           cnn_model = multispectral_CNN).squeeze(axis=0)\n        \n#         print(sal.shape)\n        plt.axis('off')\n        plt.imshow(img[:,:,1:4][:,:,::-1])\n        plt.imshow(sal,alpha=0.30,cmap='jet') #overlay\n        idx+=1\n\n        #gradcam\n        plt.subplot(4,4,idx)\n        gdcam = get_gradcam(img,\n                            score1,\n                           cnn_model = multispectral_CNN)\n        plt.imshow(img[:,:,1:4][:,:,::-1])\n        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n        plt.axis('off')\n        idx+=1\n\n\n        #gradcam ++\n        plt.subplot(4,4,idx)\n        gdcam_pls = get_gradcam_plus(img,\n                                     score1,\n                                     model = multispectral_CNN)\n        plt.imshow(img[:,:,1:4][:,:,::-1])\n        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n        plt.axis('off')\n        idx+=1\n\n        if idx>16:\n            break\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-26T11:18:35.459419Z","iopub.status.idle":"2022-09-26T11:18:35.460084Z","shell.execute_reply.started":"2022-09-26T11:18:35.459856Z","shell.execute_reply":"2022-09-26T11:18:35.45988Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model for temporal forecasting\n\n    This notebooks covered the prediction of flooding using satellite images. But this dataset is as time series dataset, which has multiple images of locations before and during the flood. So we will try to cover the temporal aspect of predicting flooding in the next notebook. This can be especially important in some cases where there are water bodies in the image, and hence there needs to be additional signal to distinguish cases of flooding ffrom normal conditions\n    \n[The temporal forecasting notebook can be found here](https://www.kaggle.com/code/virajkadam/detecting-floods-time-series-prediction)","metadata":{}},{"cell_type":"markdown","source":"# References and resources \n\n* https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B2-2020/1343/2020/isprs-archives-XLIII-B2-2020-1343-2020.pdf\n* https://arxiv.org/pdf/2006.10027v1.pdf#page=2","metadata":{}}]}