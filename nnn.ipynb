{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHJFPzkIO6_d",
        "outputId": "bda667af-2155-4772-b26e-79aee3965202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.1.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uknvz463PFcs",
        "outputId": "14183c15-a1b3-4415-a465-fc442dd183d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.27.1 which is incompatible.\n",
            "wandb 0.19.8 requires pydantic<3,>=2.6, but you have pydantic 1.9.2 which is incompatible.\n",
            "langchain-core 0.3.47 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.9.2 which is incompatible.\n",
            "geopandas 1.0.1 requires shapely>=2.0.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
            "albumentations 2.0.5 requires pydantic>=2.9.2, but you have pydantic 1.9.2 which is incompatible.\n",
            "sphinx 8.2.3 requires requests>=2.30.0, but you have requests 2.27.1 which is incompatible.\n",
            "langchain 0.3.21 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.9.2 which is incompatible.\n",
            "yfinance 0.2.55 requires requests>=2.31, but you have requests 2.27.1 which is incompatible.\n",
            "google-genai 1.7.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.9.2 which is incompatible.\n",
            "google-genai 1.7.0 requires requests<3.0.0,>=2.28.1, but you have requests 2.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install radiant_mlhub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "Lbdq-w1ARaAH",
        "outputId": "67b660cc-bab3-41b6-b829-83acd7de7577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.0.2)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.2.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (3.7.1)\n",
            "Collecting shapely>=2.0.0 (from geopandas)\n",
            "  Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.17.0)\n",
            "Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shapely\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: Shapely 1.8.5.post1\n",
            "    Uninstalling Shapely-1.8.5.post1:\n",
            "      Successfully uninstalled Shapely-1.8.5.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "radiant-mlhub 0.5.5 requires shapely~=1.8.0, but you have shapely 2.0.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed shapely-2.0.7\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b7048d123cb745bd801fe2aa799bc558",
              "pip_warning": {
                "packages": [
                  "shapely"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVREMBlgRltb",
        "outputId": "c8a1b77b-4671-4e69-e267-b5cb11adacc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement kaggle_secrets (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for kaggle_secrets\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Vs6_wtqVRTMI",
        "outputId": "334210c6-3eb0-494a-b89d-1c71429909f7"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c04673a1ef99>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#get key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0muser_secrets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msecret_value_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RadiantEarth_MLHub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kaggle/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\u001b[0m in \u001b[0;36mauthenticate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         raise IOError('Could not find {}. Make sure it\\'s located in'\n\u001b[0m\u001b[1;32m    434\u001b[0m                       \u001b[0;34m' {}. Or use the environment method. See setup'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                       \u001b[0;34m' instructions at'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from radiant_mlhub import get_session\n",
        "from radiant_mlhub import Dataset,Collection\n",
        "import shutil\n",
        "import geopandas as gpd\n",
        "\n",
        "#get key\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"RadiantEarth_MLHub\")\n",
        "\n",
        "\n",
        "#authenticate\n",
        "os.environ['MLHUB_API_KEY'] = secret_value_0\n",
        "session = get_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-hQoQ3PNZPw"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAVLcOt-NsOj",
        "outputId": "3074d424-d435-4b75-efe5-de24ce238b2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.1.31)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.1)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:17:31.571947Z",
          "iopub.status.busy": "2022-09-26T11:17:31.571406Z",
          "iopub.status.idle": "2022-09-26T11:17:39.185513Z",
          "shell.execute_reply": "2022-09-26T11:17:39.184424Z",
          "shell.execute_reply.started": "2022-09-26T11:17:31.571862Z"
        },
        "id": "8QshKZ_pNZPx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import gc\n",
        "import rasterio as rio\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import  cm\n",
        "import cv2\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXudhdoyNZPx"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:17:39.190946Z",
          "iopub.status.busy": "2022-09-26T11:17:39.189801Z",
          "iopub.status.idle": "2022-09-26T11:17:39.199337Z",
          "shell.execute_reply": "2022-09-26T11:17:39.198354Z",
          "shell.execute_reply.started": "2022-09-26T11:17:39.1909Z"
        },
        "id": "lLDwrs1GNZPy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    seed = 7\n",
        "    img_size = (256,256)\n",
        "    BATCH_SIZE = 3\n",
        "    Autotune = tf.data.AUTOTUNE\n",
        "    validation_size = 0.2\n",
        "    class_dict= {0:'No Flooding',\n",
        "                 1: 'Flooding'}\n",
        "\n",
        "    test_run = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4MXi86YNZPy"
      },
      "source": [
        "# Input data\n",
        "\n",
        "    Read more about the dataset here : https://clmrmb.github.io/SEN12-FLOOD/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:17:39.20534Z",
          "iopub.status.busy": "2022-09-26T11:17:39.203772Z",
          "iopub.status.idle": "2022-09-26T11:17:44.627438Z",
          "shell.execute_reply": "2022-09-26T11:17:44.626527Z",
          "shell.execute_reply.started": "2022-09-26T11:17:39.205252Z"
        },
        "id": "0J1Ok11cNZPy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "s1_labels = '../input/sen12flood/sen12flood/sen12floods_s1_labels/sen12floods_s1_labels/'\n",
        "s1_tiles = '../input/sen12flood/sen12flood/sen12floods_s1_source/sen12floods_s1_source/'\n",
        "\n",
        "s2_tiles = '../input/sen12flood/sen12flood/sen12floods_s2_source/sen12floods_s2_source/'\n",
        "s2_labels = '../input/sen12flood/sen12flood/sen12floods_s2_labels/sen12floods_s2_labels/'\n",
        "\n",
        "\n",
        "s1_check = 0\n",
        "for file in os.listdir(s1_labels):\n",
        "    if os.path.exists(s1_tiles + '/' + file.replace('labels','source')):\n",
        "        s1_check += 1\n",
        "\n",
        "\n",
        "assert s1_check == len(os.listdir(s1_tiles)), 'You my friend , are definintely a idiot!'\n",
        "\n",
        "s2_check = 0\n",
        "for file in os.listdir(s2_labels):\n",
        "    if os.path.exists(s2_tiles + '/' + file.replace('labels','source')):\n",
        "        s2_check += 1\n",
        "\n",
        "\n",
        "assert s2_check == len(os.listdir(s2_tiles)), 'You my friend , are definintely  the idiot!'\n",
        "\n",
        "\n",
        "s1_check,s2_check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU3UyLTHNZPy"
      },
      "source": [
        "# Make a dataset of paths and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YpZWe--NZPz"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:17:44.630098Z",
          "iopub.status.busy": "2022-09-26T11:17:44.62955Z",
          "iopub.status.idle": "2022-09-26T11:17:44.637468Z",
          "shell.execute_reply": "2022-09-26T11:17:44.636754Z",
          "shell.execute_reply.started": "2022-09-26T11:17:44.630059Z"
        },
        "id": "CepytPTENZPz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_json(path):\n",
        "    '''loads a json file'''\n",
        "    with open(path,'r') as file:\n",
        "        js = json.load(file)\n",
        "\n",
        "    return js\n",
        "\n",
        "# collectionss1 = load_json('../input/sen12flood/sen12flood/sen12floods_s1_source/sen12floods_s1_source/collection.json')\n",
        "# collections2= load_json('../input/sen12flood/sen12flood/sen12floods_s2_source/sen12floods_s2_source/collection.json')\n",
        "# collections2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:17:44.643287Z",
          "iopub.status.busy": "2022-09-26T11:17:44.642764Z",
          "iopub.status.idle": "2022-09-26T11:17:44.682133Z",
          "shell.execute_reply": "2022-09-26T11:17:44.681405Z",
          "shell.execute_reply.started": "2022-09-26T11:17:44.64325Z"
        },
        "id": "8IuG-BT7NZPz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def process_label_json(label_json):\n",
        "    '''process a single label json'''\n",
        "    info_dict = {}\n",
        "\n",
        "    info_dict['geometry'] = label_json['geometry']['coordinates']\n",
        "    info_dict['label'] = label_json['properties']['FLOODING']\n",
        "    info_dict['date'] = label_json['properties']['date']\n",
        "    info_dict['tile_number'] = label_json['properties']['tile']\n",
        "#     info_dict['full_data_coverage']= label_json['properties']['FULL-DATA-COVERAGE']\n",
        "\n",
        "    return info_dict\n",
        "\n",
        "\n",
        "def process_label_stac(stac_json):\n",
        "    return stac_json['id']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def image_path_from_label_dir(image_parent_dir:str,\n",
        "                              label_file :str)->str:\n",
        "\n",
        "    return image_parent_dir + '/' + label_file.replace('labels','source')\n",
        "\n",
        "\n",
        "\n",
        "def process_json(label_path,image_directory):\n",
        "    '''get the data for a single example\n",
        "     Inputs\n",
        "     label_path : path to the label folder\n",
        "     image_directory: path to the corresponding image directory'''\n",
        "\n",
        "\n",
        "\n",
        "    #get image directory for that label\n",
        "    folder_id = label_path.rsplit('/',1)[1]\n",
        "    image_dir_path = image_path_from_label_dir(image_directory,folder_id)\n",
        "\n",
        "    if not os.path.exists(image_dir_path):\n",
        "        return {'File_not_found':image_dir_path}\n",
        "\n",
        "\n",
        "    for file in os.listdir(label_path):\n",
        "        #if image dir exists\n",
        "        if file.startswith('labels'):\n",
        "            label_json = load_json(os.path.join(label_path,file))\n",
        "        else:\n",
        "            stac_json = load_json(os.path.join(label_path,file))\n",
        "\n",
        "\n",
        "    #get data\n",
        "    info_dict = process_label_json(label_json)\n",
        "\n",
        "    #get id\n",
        "    info_dict['id'] = process_label_stac(stac_json)\n",
        "\n",
        "    #location id\n",
        "    info_dict['location_id'] = info_dict['id'].split('_')[3]\n",
        "\n",
        "\n",
        "    info_dict['image_dir'] = image_dir_path\n",
        "\n",
        "\n",
        "    return info_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:17:44.684094Z",
          "iopub.status.busy": "2022-09-26T11:17:44.683534Z",
          "iopub.status.idle": "2022-09-26T11:17:44.697387Z",
          "shell.execute_reply": "2022-09-26T11:17:44.696614Z",
          "shell.execute_reply.started": "2022-09-26T11:17:44.684058Z"
        },
        "id": "xP-cYoSCNZP0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_dataframe(label_directory,image_directory):\n",
        "    '''get dataframe from the nested label directory'''\n",
        "    records = []\n",
        "\n",
        "\n",
        "    for folder in os.listdir(label_directory):\n",
        "        if folder.startswith('sen12'):\n",
        "#             print(folder,label_directory)\n",
        "            folder_path = label_directory + '/' + folder\n",
        "\n",
        "\n",
        "            #get data for a single example\n",
        "            feature = process_json(label_path=folder_path,\n",
        "                                   image_directory=image_directory)\n",
        "\n",
        "\n",
        "            records.append(feature)\n",
        "\n",
        "\n",
        "    return pd.DataFrame.from_records(data = records)\n",
        "\n",
        "\n",
        "\n",
        "def type_cast_dataset(dataset):\n",
        "    '''typecasting columns in dataset'''\n",
        "    dataset['label'] = dataset['label'].astype(int)\n",
        "\n",
        "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "    dataset['tile_number'] = dataset['tile_number'].astype('int8')\n",
        "\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:17:44.699232Z",
          "iopub.status.busy": "2022-09-26T11:17:44.698688Z",
          "iopub.status.idle": "2022-09-26T11:18:26.758297Z",
          "shell.execute_reply": "2022-09-26T11:18:26.757493Z",
          "shell.execute_reply.started": "2022-09-26T11:17:44.699198Z"
        },
        "id": "QuyS1tg9NZP0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "s1_data = type_cast_dataset(\n",
        "                            get_dataframe(\n",
        "                                label_directory=s1_labels,\n",
        "                                image_directory=s1_tiles\n",
        "                                        )\n",
        "                            )\n",
        "\n",
        "\n",
        "s2_data = type_cast_dataset(\n",
        "                            get_dataframe(label_directory=s2_labels,\n",
        "                                          image_directory=s2_tiles)\n",
        "                            )\n",
        "\n",
        "print(f'Number of unique locations in Sentinel1 (SAR) data : {s1_data.location_id.nunique()}')\n",
        "print(f'Number of unique locations in Sentinel2 (optical) data : {s2_data.location_id.nunique()}')\n",
        "\n",
        "s1_data.shape,s2_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:18:26.759984Z",
          "iopub.status.busy": "2022-09-26T11:18:26.759627Z",
          "iopub.status.idle": "2022-09-26T11:18:26.855362Z",
          "shell.execute_reply": "2022-09-26T11:18:26.854592Z",
          "shell.execute_reply.started": "2022-09-26T11:18:26.759949Z"
        },
        "id": "yELlSPPXNZP0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# saving datasets\n",
        "s1_data.to_csv('s1_data.csv',index=False)\n",
        "s2_data.to_csv('s2_data.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:25.444543Z",
          "iopub.status.busy": "2022-09-26T11:20:25.444184Z",
          "iopub.status.idle": "2022-09-26T11:20:25.448955Z",
          "shell.execute_reply": "2022-09-26T11:20:25.448119Z",
          "shell.execute_reply.started": "2022-09-26T11:20:25.444512Z"
        },
        "id": "teymr0vsNZP0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_raster(filepath):\n",
        "    '''load a single band raster'''\n",
        "    with rio.open(filepath) as file:\n",
        "        raster = file.read().squeeze(axis=0)\n",
        "\n",
        "    return raster\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xzCdDRNZP0"
      },
      "source": [
        "**Loading multiple raster bands as single raster**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:26.789549Z",
          "iopub.status.busy": "2022-09-26T11:20:26.789188Z",
          "iopub.status.idle": "2022-09-26T11:20:26.803006Z",
          "shell.execute_reply": "2022-09-26T11:20:26.80203Z",
          "shell.execute_reply.started": "2022-09-26T11:20:26.789519Z"
        },
        "id": "c4ylAbVaNZP1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_s1_tiffs(folder,\n",
        "                  scaling_values=[50.,100.]):\n",
        "    images = []\n",
        "    i = 0\n",
        "    for im in sorted(os.listdir(folder)):\n",
        "\n",
        "        if im.rsplit('.',maxsplit=1)[1] == 'tif':\n",
        "\n",
        "            path = folder + '/' + im\n",
        "            band = load_raster(path)\n",
        "            band = band / scaling_values[i]\n",
        "\n",
        "            band = cv2.resize(band,\n",
        "                              CFG.img_size)\n",
        "\n",
        "            images.append(band)\n",
        "            i+=1\n",
        "\n",
        "    return np.dstack(images)\n",
        "\n",
        "\n",
        "def load_s2_tiffs(folder,\n",
        "                  scaling_value=10000.):\n",
        "    images = []\n",
        "    for im in sorted(os.listdir(folder)):\n",
        "        if im.rsplit('.',maxsplit=1)[1] == 'tif':\n",
        "            path = folder + '/' + im\n",
        "            band = load_raster(path)\n",
        "            band = band/ scaling_value\n",
        "\n",
        "            band = cv2.resize(band,CFG.img_size)\n",
        "            images.append(band)\n",
        "\n",
        "    return np.dstack(images)\n",
        "\n",
        "def load_rgb_tiffs(folder,\n",
        "                  scaling_value=10000.):\n",
        "    '''load R,G and B bands'''\n",
        "\n",
        "    images = []\n",
        "    for im in sorted(os.listdir(folder)):\n",
        "        name,file_format = im.rsplit('.',maxsplit=1)\n",
        "        if ((file_format== 'tif') and (name in ['B02','B03','B04'])):\n",
        "            path = folder + '/' + im\n",
        "            band = load_raster(path)\n",
        "            band = band/ scaling_value\n",
        "\n",
        "            band = cv2.resize(band,CFG.img_size)\n",
        "            images.append(band)\n",
        "\n",
        "    return np.dstack(images)[:,:,::-1]\n",
        "\n",
        "\n",
        "\n",
        "def tf_load_s1(path):\n",
        "    path = path.numpy().decode('utf-8')\n",
        "    return load_s1_tiffs(path)\n",
        "\n",
        "\n",
        "\n",
        "def tf_load_s2(path):\n",
        "    path = path.numpy().decode('utf-8')\n",
        "    return load_s2_tiffs(path)\n",
        "\n",
        "\n",
        "def tf_load_rgb(path):\n",
        "    path = path.numpy().decode('utf-8')\n",
        "    return load_rgb_tiffs(path)\n",
        "\n",
        "def process_image_s1(filename):\n",
        "    '''function for preprocessing in tensorflow data'''\n",
        "\n",
        "    return tf.py_function(tf_load_s1,\n",
        "                          [filename],\n",
        "                          tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "def process_image_s2(filename):\n",
        "    '''function for preprocessing in tensorflow data'''\n",
        "\n",
        "    return tf.py_function(tf_load_s2,\n",
        "                          [filename],\n",
        "                          tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "def process_image_rgb(filename):\n",
        "    '''function for preprocessing in tensorflow data'''\n",
        "\n",
        "    return tf.py_function(tf_load_rgb,\n",
        "                          [filename],\n",
        "                          tf.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:26.921047Z",
          "iopub.status.busy": "2022-09-26T11:20:26.920456Z",
          "iopub.status.idle": "2022-09-26T11:20:28.00142Z",
          "shell.execute_reply": "2022-09-26T11:20:28.000621Z",
          "shell.execute_reply.started": "2022-09-26T11:20:26.921018Z"
        },
        "id": "iC62L46HNZP1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def count_rasters_in_folder(path):\n",
        "    count = 0\n",
        "\n",
        "    for file in os.listdir(path):\n",
        "        if file.rsplit('.',1)[1] == 'tif':\n",
        "            count +=1\n",
        "\n",
        "    return count\n",
        "\n",
        "\n",
        "s2_data['raster_count'] = s2_data.image_dir.apply(lambda x : count_rasters_in_folder(x))\n",
        "\n",
        "#value counts\n",
        "s2_data['raster_count'].value_counts()\n",
        "\n",
        "\n",
        "s2_data=s2_data[s2_data['raster_count']==12] # take only valid rasters\n",
        "# s2_data[s2_data['raster_count']==0]['location_id'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWSufRcpNZP1"
      },
      "source": [
        "# Visualize some images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yaylp5qoNZP1"
      },
      "source": [
        "**lets take a look at some optical RGB images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:28.003605Z",
          "iopub.status.busy": "2022-09-26T11:20:28.002952Z",
          "iopub.status.idle": "2022-09-26T11:20:29.04393Z",
          "shell.execute_reply": "2022-09-26T11:20:29.042448Z",
          "shell.execute_reply.started": "2022-09-26T11:20:28.003551Z"
        },
        "id": "gxANZmf_NZP1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "chk = load_s2_tiffs(s2_data.query('label==1')['image_dir'].values[11])\n",
        "plt.imshow(chk[:,:,1:4][:,:,::-1])\n",
        "plt.title('Flooding')\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "chk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[20])\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(chk[:,:,1:4][:,:,::-1])\n",
        "plt.title('No Flooding')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:31.104243Z",
          "iopub.status.busy": "2022-09-26T11:20:31.103854Z",
          "iopub.status.idle": "2022-09-26T11:20:31.78512Z",
          "shell.execute_reply": "2022-09-26T11:20:31.781896Z",
          "shell.execute_reply.started": "2022-09-26T11:20:31.10421Z"
        },
        "id": "ERwkNsQyNZP1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "chk = load_s2_tiffs(s2_data.query('label==1')['image_dir'].values[101])\n",
        "plt.imshow(chk[:,:,1:4][:,:,::-1])\n",
        "plt.title('Flooding')\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "chk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[11])\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(chk[:,:,1:4][:,:,::-1])\n",
        "plt.title('No Flooding')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE1f0KfGNZP2"
      },
      "source": [
        "**Some SAR images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:34.677069Z",
          "iopub.status.busy": "2022-09-26T11:20:34.676701Z",
          "iopub.status.idle": "2022-09-26T11:20:35.314161Z",
          "shell.execute_reply": "2022-09-26T11:20:35.313475Z",
          "shell.execute_reply.started": "2022-09-26T11:20:34.677039Z"
        },
        "id": "PbT8rXFiNZP2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "plt.suptitle('SAR images (VH)')\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "\n",
        "\n",
        "chk = load_s1_tiffs(s1_data.query('label==1')['image_dir'].values[11])\n",
        "plt.imshow(chk[:,:,1])\n",
        "plt.title('Flooding')\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "chk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[11])\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(chk[:,:,1])\n",
        "plt.title('No Flooding')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:35.316316Z",
          "iopub.status.busy": "2022-09-26T11:20:35.315489Z",
          "iopub.status.idle": "2022-09-26T11:20:35.834857Z",
          "shell.execute_reply": "2022-09-26T11:20:35.834083Z",
          "shell.execute_reply.started": "2022-09-26T11:20:35.316281Z"
        },
        "id": "RXAehgx-NZP2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,8))\n",
        "plt.suptitle('SAR images (VV)')\n",
        "\n",
        "\n",
        "r_idx = np.random.randint(low=0,high=500)\n",
        "plt.subplot(1,2,1)\n",
        "chk = load_s1_tiffs(s1_data.query('label==1')['image_dir'].values[r_idx])\n",
        "plt.imshow(chk[:,:,0])\n",
        "plt.title('Flooding')\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "chk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[r_idx])\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(chk[:,:,0])\n",
        "plt.title('No Flooding')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:35.837108Z",
          "iopub.status.busy": "2022-09-26T11:20:35.83656Z",
          "iopub.status.idle": "2022-09-26T11:20:36.550042Z",
          "shell.execute_reply": "2022-09-26T11:20:36.549348Z",
          "shell.execute_reply.started": "2022-09-26T11:20:35.837069Z"
        },
        "id": "YvLSaTmINZP2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,8))\n",
        "plt.suptitle('SAR images (VV)')\n",
        "\n",
        "\n",
        "r_idx = np.random.randint(low=0,high=500)\n",
        "plt.subplot(1,2,1)\n",
        "chk = load_s1_tiffs(s1_data.query('label==1')['image_dir'].values[r_idx])\n",
        "plt.imshow(chk[:,:,0])\n",
        "plt.title('Flooding')\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "chk = load_s2_tiffs(s2_data.query('label==0')['image_dir'].values[r_idx])\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(chk[:,:,0])\n",
        "plt.title('No Flooding')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEumqNhgNZP2"
      },
      "source": [
        "**Lets look at the distribution of target values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:38.183059Z",
          "iopub.status.busy": "2022-09-26T11:20:38.182366Z",
          "iopub.status.idle": "2022-09-26T11:20:38.445155Z",
          "shell.execute_reply": "2022-09-26T11:20:38.444411Z",
          "shell.execute_reply.started": "2022-09-26T11:20:38.183021Z"
        },
        "id": "mJ5ww0OrNZP2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.countplot(s1_data.label)\n",
        "plt.title('Sentinel -1 target distribution')\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.countplot(s2_data.label)\n",
        "plt.title('Sentinel - 2 target distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW5gdYrNNZP3"
      },
      "source": [
        "# Making a TF dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgvlHEVmNZP3"
      },
      "source": [
        "    First lets split the dataset into training and validation set. We will stratify based on location id to ensure that locations are well represented in traininng and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:39.135045Z",
          "iopub.status.busy": "2022-09-26T11:20:39.134677Z",
          "iopub.status.idle": "2022-09-26T11:20:39.150722Z",
          "shell.execute_reply": "2022-09-26T11:20:39.149907Z",
          "shell.execute_reply.started": "2022-09-26T11:20:39.135013Z"
        },
        "id": "bLGXUuUQNZP3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#isolating single loaction ids (as they will be a problem for stratification)\n",
        "\n",
        "# single example locations\n",
        "single_index = s2_data['location_id'].value_counts()[s2_data['location_id'].value_counts()==1].index\n",
        "\n",
        "single_index_df = s2_data[s2_data['location_id'].isin(single_index)].reset_index(drop=True)\n",
        "s2_data0 = s2_data[~(s2_data['location_id'].isin(single_index))].reset_index(drop=True)\n",
        "\n",
        "s2_data0.shape,single_index_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldtrRQD1NZP3"
      },
      "source": [
        "**Split dataset into train and validation splits**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:39.958183Z",
          "iopub.status.busy": "2022-09-26T11:20:39.957736Z",
          "iopub.status.idle": "2022-09-26T11:20:40.250023Z",
          "shell.execute_reply": "2022-09-26T11:20:40.247659Z",
          "shell.execute_reply.started": "2022-09-26T11:20:39.958142Z"
        },
        "id": "gzhQKwBANZP3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#traintest split\n",
        "\n",
        "s1_data_tr,s1_data_val= train_test_split(s1_data,\n",
        "                                          test_size = CFG.validation_size,\n",
        "                                          random_state = CFG.seed,\n",
        "                                          stratify = s1_data.location_id)\n",
        "\n",
        "\n",
        "\n",
        "s2_data_tr,s2_data_val = train_test_split(s2_data0,\n",
        "                                          test_size = CFG.validation_size,\n",
        "                                          random_state = CFG.seed,\n",
        "                                          stratify =  s2_data0.location_id)\n",
        "\n",
        "s2_data_tr = s2_data_tr.append(single_index_df,ignore_index=True)\n",
        "\n",
        "del s2_data0;gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:41.042803Z",
          "iopub.status.busy": "2022-09-26T11:20:41.042192Z",
          "iopub.status.idle": "2022-09-26T11:20:41.05281Z",
          "shell.execute_reply": "2022-09-26T11:20:41.051772Z",
          "shell.execute_reply.started": "2022-09-26T11:20:41.042766Z"
        },
        "id": "8h1K57CTNZP3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "s1_data_tr.label.value_counts(1),s1_data_val.label.value_counts(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:41.184518Z",
          "iopub.status.busy": "2022-09-26T11:20:41.184185Z",
          "iopub.status.idle": "2022-09-26T11:20:41.194118Z",
          "shell.execute_reply": "2022-09-26T11:20:41.19315Z",
          "shell.execute_reply.started": "2022-09-26T11:20:41.184489Z"
        },
        "id": "SwSNYJwDNZP4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "s2_data_tr.label.value_counts(1),s2_data_val.label.value_counts(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O4S2cJLNZP4"
      },
      "source": [
        "**Function for image augmentations**\n",
        "\n",
        "    Although the Augmentations are simple, we cannot use them on SAR images , as even simple operations like flipping can change the meaning of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:49.334259Z",
          "iopub.status.busy": "2022-09-26T11:20:49.333889Z",
          "iopub.status.idle": "2022-09-26T11:20:49.341871Z",
          "shell.execute_reply": "2022-09-26T11:20:49.340804Z",
          "shell.execute_reply.started": "2022-09-26T11:20:49.334227Z"
        },
        "id": "21jvzJP8NZP4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def augment_image_multispectral(image):\n",
        "    '''perform simple image augmentations'''\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_crop(image, size=(*CFG.img_size,12))\n",
        "\n",
        "    rot = tf.random.normal((1,),mean = 0.35, stddev=0.15)\n",
        "\n",
        "    if rot > 0.5:\n",
        "        image = tf.image.rot90(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "def augment_image(image):\n",
        "    '''perform simple image augmentations'''\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_crop(image, size=(*CFG.img_size,3))\n",
        "\n",
        "    rot = tf.random.normal((1,),mean = 0.35, stddev=0.15)\n",
        "\n",
        "    if rot > 0.5:\n",
        "        image = tf.image.rot90(image)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:50.67682Z",
          "iopub.status.busy": "2022-09-26T11:20:50.675989Z",
          "iopub.status.idle": "2022-09-26T11:20:50.685098Z",
          "shell.execute_reply": "2022-09-26T11:20:50.68435Z",
          "shell.execute_reply.started": "2022-09-26T11:20:50.676786Z"
        },
        "id": "i1x7Lpa9NZP5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_tf_dataset(image_paths,\n",
        "                   labels=None, # put none for test data set\n",
        "                   image_processing_fn=None,\n",
        "                   augment_fn = None\n",
        "                  ):\n",
        "\n",
        "    '''returns a tf dataset object\n",
        "    Inputs:\n",
        "    image_paths : paths to images\n",
        "    labels: labels of each image\n",
        "    image_processing_fn:  function to load and preprocess images\n",
        "    augment_fn : function to augment images '''\n",
        "\n",
        "    #seperate datasets\n",
        "    if labels is not None:\n",
        "        labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "\n",
        "\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    #load images\n",
        "    image_dataset = image_dataset.map(image_processing_fn,\n",
        "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment_fn is not None:\n",
        "\n",
        "        image_dataset = image_dataset.map(augment_fn,\n",
        "                                          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "    if labels is not None:\n",
        "        return tf.data.Dataset.zip((image_dataset,labels_dataset))\n",
        "\n",
        "\n",
        "    return image_dataset\n",
        "\n",
        "\n",
        "\n",
        "def optimize_pipeline(tf_dataset,\n",
        "                      batch_size = CFG.BATCH_SIZE,\n",
        "                      Autotune_fn = CFG.Autotune,\n",
        "                      cache= False,\n",
        "                      batch = True):\n",
        "\n",
        "\n",
        "\n",
        "    # prefetch(load the data with cpu,while gpu is training) the data in memory\n",
        "    tf_dataset = tf_dataset.prefetch(buffer_size=Autotune_fn)\n",
        "    if cache:\n",
        "        tf_dataset = tf_dataset.cache()                        # store data in RAM\n",
        "\n",
        "    tf_dataset =  tf_dataset.shuffle(buffer_size=50)         # shuffle\n",
        "\n",
        "    if batch:\n",
        "        tf_dataset = tf_dataset.batch(batch_size)              #split the data in batches\n",
        "\n",
        "    return tf_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demXYmMDNZP5"
      },
      "source": [
        "**Making dataset pipelines with TF data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:51.710133Z",
          "iopub.status.busy": "2022-09-26T11:20:51.709776Z",
          "iopub.status.idle": "2022-09-26T11:20:54.658028Z",
          "shell.execute_reply": "2022-09-26T11:20:54.657145Z",
          "shell.execute_reply.started": "2022-09-26T11:20:51.710102Z"
        },
        "id": "W69Qy0eiNZP5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Sentinel 1 dataset (not using augmentation here)\n",
        "\n",
        "S1_dataset_tr = optimize_pipeline(tf_dataset=get_tf_dataset(image_paths = s1_data_tr.image_dir.values,\n",
        "                                               labels = s1_data_tr.label,\n",
        "                                               image_processing_fn = process_image_s1),\n",
        "\n",
        "                                  batch_size = 3 * CFG.BATCH_SIZE)\n",
        "\n",
        "\n",
        "S1_dataset_val = optimize_pipeline(tf_dataset = get_tf_dataset(image_paths = s1_data_val.image_dir.values,\n",
        "                                                           labels = s1_data_val.label,\n",
        "                                                           image_processing_fn = process_image_s1 ),\n",
        "                                   batch_size = 3* CFG.BATCH_SIZE\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:55.828238Z",
          "iopub.status.busy": "2022-09-26T11:20:55.827872Z",
          "iopub.status.idle": "2022-09-26T11:20:56.203608Z",
          "shell.execute_reply": "2022-09-26T11:20:56.202775Z",
          "shell.execute_reply.started": "2022-09-26T11:20:55.828207Z"
        },
        "id": "F2fMHjGxNZP5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#sentinel 2 dataset\n",
        "S2_dataset_tr = optimize_pipeline(get_tf_dataset(image_paths = s2_data_tr.image_dir.values,\n",
        "                                                   labels = s2_data_tr.label,\n",
        "                                                   image_processing_fn = process_image_s2,\n",
        "                                                   augment_fn = augment_image_multispectral)\n",
        "                                 )\n",
        "\n",
        "\n",
        "S2_dataset_val = optimize_pipeline(get_tf_dataset(image_paths = s2_data_val.image_dir.values,\n",
        "                                                   labels = s2_data_val.label,\n",
        "                                                   image_processing_fn = process_image_s2,\n",
        "                                                   augment_fn = augment_image_multispectral)\n",
        "                                  )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:56.587354Z",
          "iopub.status.busy": "2022-09-26T11:20:56.586846Z",
          "iopub.status.idle": "2022-09-26T11:20:56.957337Z",
          "shell.execute_reply": "2022-09-26T11:20:56.956546Z",
          "shell.execute_reply.started": "2022-09-26T11:20:56.587319Z"
        },
        "id": "2Fn9nDh0NZP6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "RGB_dataset_tr = optimize_pipeline(get_tf_dataset(image_paths = s2_data_tr.image_dir.values,\n",
        "                                                   labels = s2_data_tr.label,\n",
        "                                                   image_processing_fn = process_image_rgb,\n",
        "                                                   augment_fn = augment_image),\n",
        "                                   batch_size = 3* CFG.BATCH_SIZE\n",
        "                                 )\n",
        "\n",
        "\n",
        "RGB_dataset_val = optimize_pipeline(get_tf_dataset(image_paths = s2_data_val.image_dir.values,\n",
        "                                                   labels = s2_data_val.label,\n",
        "                                                   image_processing_fn = process_image_rgb,\n",
        "                                                   augment_fn = augment_image),\n",
        "                                    batch_size = 3* CFG.BATCH_SIZE\n",
        "                                  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY2gpfAeNZP6"
      },
      "source": [
        "**Checking the values of pixels in the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2022-09-26T11:18:35.414711Z",
          "iopub.status.idle": "2022-09-26T11:18:35.415401Z",
          "shell.execute_reply": "2022-09-26T11:18:35.415197Z",
          "shell.execute_reply.started": "2022-09-26T11:18:35.415172Z"
        },
        "id": "cfSlFesONZP6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# max_vals_vv = []\n",
        "# mean_vals_vv =[]\n",
        "\n",
        "\n",
        "# max_vals_vh = []\n",
        "# mean_vals_vh =[]\n",
        "\n",
        "# for x,y in S1_dataset_tr.as_numpy_iterator():\n",
        "#     # vv band\n",
        "#     max_vals_vv.append(x[:,:,:,0].max()); mean_vals_vv.append(x[:,:,:,0].mean())\n",
        "\n",
        "#     # vh band\n",
        "#     max_vals_vh.append(x[:,:,:,1].max()); mean_vals_vh.append(x[:,:,:,1].mean())\n",
        "\n",
        "# # band 1value distributions\n",
        "# plt.figure(figsize=(16,10))\n",
        "\n",
        "# sns.distplot(max_vals_vv,label = 'VV band Max values',color='b')\n",
        "# # sns.distplot(mean_vals_vv,label = 'VV band Mean values',color = 'g')\n",
        "\n",
        "# plt.legend()\n",
        "\n",
        "# plt.title('VV band values Distribution')\n",
        "# plt.show()\n",
        "\n",
        "# # band 1value distributions\n",
        "# plt.figure(figsize=(16,10))\n",
        "\n",
        "# sns.distplot(max_vals_vh,label = 'VH band Max values',color='b')\n",
        "# # sns.distplot(mean_vals_vh,label = 'VH band Mean values',color = 'g')\n",
        "\n",
        "\n",
        "# plt.title('VH band values Distribution')\n",
        "\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dzfU2StNZP6"
      },
      "source": [
        "**Checking to see if the Pipelines work as expected**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:21:03.786771Z",
          "iopub.status.busy": "2022-09-26T11:21:03.786345Z",
          "iopub.status.idle": "2022-09-26T11:21:05.336129Z",
          "shell.execute_reply": "2022-09-26T11:21:05.335191Z",
          "shell.execute_reply.started": "2022-09-26T11:21:03.786734Z"
        },
        "id": "tcfbi3HUNZP6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for x,y in S1_dataset_tr.take(1): # take one batch for checking\n",
        "    print(f'shape of SAR dataset input {x.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:21:05.338245Z",
          "iopub.status.busy": "2022-09-26T11:21:05.337856Z",
          "iopub.status.idle": "2022-09-26T11:21:09.84121Z",
          "shell.execute_reply": "2022-09-26T11:21:09.839157Z",
          "shell.execute_reply.started": "2022-09-26T11:21:05.338204Z"
        },
        "id": "4ZXigL3INZP6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for x,y in S2_dataset_tr.take(1): # take one batch for checking\n",
        "    print(f'shape of MultiSpectral dataset input {x.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:21:09.844893Z",
          "iopub.status.busy": "2022-09-26T11:21:09.844502Z",
          "iopub.status.idle": "2022-09-26T11:21:11.083431Z",
          "shell.execute_reply": "2022-09-26T11:21:11.082676Z",
          "shell.execute_reply.started": "2022-09-26T11:21:09.844863Z"
        },
        "id": "u1PQdk_nNZP7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for x,y in RGB_dataset_tr.take(1): # take one batch for checking\n",
        "    print(f'shape of MultiSpectral dataset input {x.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwd6_3PSNZP7"
      },
      "source": [
        "#  CNN Models\n",
        "    CNN models to identify flooding in opotical and SAR images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:21:17.585362Z",
          "iopub.status.busy": "2022-09-26T11:21:17.584995Z",
          "iopub.status.idle": "2022-09-26T11:21:17.604232Z",
          "shell.execute_reply": "2022-09-26T11:21:17.602103Z",
          "shell.execute_reply.started": "2022-09-26T11:21:17.585328Z"
        },
        "id": "A_FqRwwENZP7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def multichannel_cnn(num_channels:int,\n",
        "                     hidden_units:int, #number of  hidden dense\n",
        "                     weights = None  # none for random init, use imagenet for imagenet weights\n",
        "                    ):\n",
        "    '''model that takes multiple channel as input, instead of using the rgb channels as by default'''\n",
        "\n",
        "\n",
        "    backbone = tf.keras.applications.resnet_v2.ResNet50V2(\n",
        "                                            include_top=False,\n",
        "                                            input_shape = (*CFG.img_size,num_channels),\n",
        "                                            weights=weights,\n",
        "                                            pooling = 'avg')\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.BatchNormalization()(backbone.output)\n",
        "    x = tf.keras.layers.Dense(hidden_units,\n",
        "                              activation = 'relu')(x)\n",
        "\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(rate = 0.2)(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    final_out = tf.keras.layers.Dense(2,\n",
        "                                      activation = 'softmax')(x)\n",
        "\n",
        "\n",
        "    #make a model\n",
        "    model = tf.keras.Model(inputs = backbone.input,\n",
        "                  outputs = final_out)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# plot train and val acc as  a function of epochs\n",
        "def plot_history(history,addn_metric=None):\n",
        "    '''\n",
        "    Inputs\n",
        "    history:history object from tensorflow\n",
        "    add_metric: metric name in the history (like f1_score)'''\n",
        "    his=pd.DataFrame(history.history)\n",
        "\n",
        "    if addn_metric:\n",
        "        plt.subplots(1,3,figsize=(20,6))\n",
        "\n",
        "        #loss:\n",
        "        ax1=plt.subplot(1,3,1)\n",
        "        ax1.plot(range(len(his)),his['loss'],color='g',label='training')\n",
        "        ax1.plot(range(len(his)),his['val_loss'],color='r',label='validation')\n",
        "        ax1.set_xlabel('EPOCHS')\n",
        "        ax1.set_ylabel('LOSS')\n",
        "        ax1.legend()\n",
        "        ax1.set_title('Loss Per Epoch')\n",
        "\n",
        "        #accuracy\n",
        "        ax2=plt.subplot(1,3,2)\n",
        "        ax2.plot(range(len(his)),his['accuracy'],color='g',label='training_acc')\n",
        "        ax2.plot(range(len(his)),his['val_accuracy'],color='r',label='validation_acc')\n",
        "        ax2.set_xlabel('EPOCHS')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.legend()\n",
        "        ax2.set_title('Accuracy Per Epoch')\n",
        "\n",
        "\n",
        "\n",
        "        ax3= plt.subplot(1,3,3)\n",
        "        ax3.plot(range(len(his)),his[f'{addn_metric}'],color='g',label='training')\n",
        "        ax3.plot(range(len(his)),his[f'val_{addn_metric}'],color='r',label='validation')\n",
        "        ax3.set_xlabel('EPOCHS')\n",
        "        ax3.set_ylabel(f'{addn_metric}')\n",
        "        ax3.legend()\n",
        "        ax3.set_title(f'{addn_metric} Per Epoch')\n",
        "\n",
        "\n",
        "    else:\n",
        "        plt.subplots(1,2,figsize=(20,8))\n",
        "\n",
        "\n",
        "\n",
        "        #loss:\n",
        "        ax1=plt.subplot(1,2,1)\n",
        "        ax1.plot(range(len(his)),his['loss'],color='g',label='training')\n",
        "        ax1.plot(range(len(his)),his['val_loss'],color='r',label='validation')\n",
        "        ax1.set_xlabel('EPOCHS')\n",
        "        ax1.set_ylabel('LOSS')\n",
        "        ax1.legend()\n",
        "        ax1.set_title('Loss Per Epoch')\n",
        "\n",
        "        #accuracy\n",
        "        ax2=plt.subplot(1,2,2)\n",
        "        ax2.plot(range(len(his)),his['accuracy'],color='g',label='training_acc')\n",
        "        ax2.plot(range(len(his)),his['val_accuracy'],color='r',label='validation_acc')\n",
        "        ax2.set_xlabel('EPOCHS')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.legend()\n",
        "        ax2.set_title('Accuracy Per Epoch')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NtOGIjJNZP7"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:21:19.20648Z",
          "iopub.status.busy": "2022-09-26T11:21:19.206112Z",
          "iopub.status.idle": "2022-09-26T11:21:19.213861Z",
          "shell.execute_reply": "2022-09-26T11:21:19.212908Z",
          "shell.execute_reply.started": "2022-09-26T11:21:19.206449Z"
        },
        "id": "N-ViVq-zNZP7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#from https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QQ4lVQhNZP7"
      },
      "source": [
        "# Building and training the SAR CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:21:22.263567Z",
          "iopub.status.busy": "2022-09-26T11:21:22.263209Z",
          "iopub.status.idle": "2022-09-26T11:21:24.105606Z",
          "shell.execute_reply": "2022-09-26T11:21:24.104257Z",
          "shell.execute_reply.started": "2022-09-26T11:21:22.263536Z"
        },
        "id": "HgDhu_2UNZP7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "SAR_CNN = multichannel_cnn(num_channels = 2,\n",
        "                           hidden_units = 512, #number of  hidden dense\n",
        "                          )\n",
        "\n",
        "\n",
        "\n",
        "SAR_CNN.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss = 'sparse_categorical_crossentropy',\n",
        "                metrics = ['accuracy',f1_score,recall_m,precision_m]\n",
        "               )\n",
        "\n",
        "#check on some data\n",
        "# SAR_CNN(x)\n",
        "\n",
        "\n",
        "\n",
        "!mkdir CNN_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL5lk_s3NZP8"
      },
      "source": [
        "# Defining callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:21:25.560948Z",
          "iopub.status.busy": "2022-09-26T11:21:25.560504Z",
          "iopub.status.idle": "2022-09-26T11:21:25.570248Z",
          "shell.execute_reply": "2022-09-26T11:21:25.568987Z",
          "shell.execute_reply.started": "2022-09-26T11:21:25.560912Z"
        },
        "id": "JaAhb7k8NZP8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "EPOCHS = 2 if CFG.test_run else 75\n",
        "# callbacks\n",
        "#reduce_learning rate\n",
        "reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(patience=3,\n",
        "                                                factor=0.75,\n",
        "                                                min_delta=1e-2,\n",
        "                                                monitor='val_accuracy',\n",
        "                                                verbose=1,\n",
        "                                                mode='max')\n",
        "\n",
        "#early stopping\n",
        "early_stopping=tf.keras.callbacks.EarlyStopping(patience=15,\n",
        "                                              min_delta=1e-3,\n",
        "                                              monitor='val_accuracy',\n",
        "                                              restore_best_weights=True,\n",
        "                                              mode='max')\n",
        "\n",
        "\n",
        "# exponential decay\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "    '''learning rate scheduler, decays expo after the tenth epoch'''\n",
        "\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "\n",
        "\n",
        "learning_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "\n",
        "callbacks_1= [reduce_lr,early_stopping,learning_scheduler]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sHznSVeNZP8"
      },
      "source": [
        "**Training on SAR data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T11:21:33.585117Z",
          "iopub.status.busy": "2022-09-26T11:21:33.584743Z",
          "iopub.status.idle": "2022-09-26T11:55:12.4072Z",
          "shell.execute_reply": "2022-09-26T11:55:12.406424Z",
          "shell.execute_reply.started": "2022-09-26T11:21:33.585085Z"
        },
        "id": "41WVKWDVNZP8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "hist1 = SAR_CNN.fit(S1_dataset_tr,\n",
        "                    validation_data = S1_dataset_val,\n",
        "                    epochs = EPOCHS,\n",
        "                    callbacks = callbacks_1\n",
        "                   )\n",
        "\n",
        "\n",
        "#save model\n",
        "sar_model_path = 'CNN_models/SAR_CNN.h5'\n",
        "SAR_CNN.save(filepath = 'CNN_models/SAR_CNN.h5')\n",
        "\n",
        "\n",
        "#plot history\n",
        "plot_history(hist1,'f1_score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57yok-D0NZP8"
      },
      "source": [
        "****Evaluate on validation dataset****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T12:02:01.484252Z",
          "iopub.status.busy": "2022-09-26T12:02:01.483411Z",
          "iopub.status.idle": "2022-09-26T12:02:08.769167Z",
          "shell.execute_reply": "2022-09-26T12:02:08.768221Z",
          "shell.execute_reply.started": "2022-09-26T12:02:01.484213Z"
        },
        "id": "y8AN_enkNZP8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "SAR_CNN.evaluate(S1_dataset_val)\n",
        "\n",
        "del SAR_CNN;gc.collect()\n",
        "\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhn8Cyk2NZP9"
      },
      "source": [
        "# Model explainablity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KlbmT_NZP9"
      },
      "source": [
        "    Saliency and Grad Cam. Lets look at what the model is looking at while making htat decision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T12:02:15.313825Z",
          "iopub.status.busy": "2022-09-26T12:02:15.312955Z",
          "iopub.status.idle": "2022-09-26T12:02:27.266161Z",
          "shell.execute_reply": "2022-09-26T12:02:27.265201Z",
          "shell.execute_reply.started": "2022-09-26T12:02:15.313788Z"
        },
        "id": "rKuvn7P6NZP9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install tf_keras_vis -q\n",
        "import tf_keras_vis\n",
        "\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n",
        "\n",
        "\n",
        "tf_keras_vis.__version__\n",
        "\n",
        "\n",
        "def model_modifier_function(cloned_model):\n",
        "    '''modify model activation'''\n",
        "    cloned_model.layers[-1].activation = tf.keras.activations.linear\n",
        "\n",
        "\n",
        "def get_saliency(img,\n",
        "                 score,\n",
        "                 cnn_model,\n",
        "                 model_modifier=model_modifier_function):\n",
        "    #saliency map\n",
        "\n",
        "\n",
        "    # Create Saliency object.\n",
        "    saliency = Saliency(cnn_model,\n",
        "                        model_modifier=model_modifier_function,\n",
        "                        clone=True)\n",
        "    #saliency map\n",
        "    sal_map  = saliency(score,\n",
        "                        img,\n",
        "                        smooth_samples=20, # The number of calculating gradients iterations.\n",
        "                        smooth_noise=0.20) # noise spread level.\n",
        "    return sal_map\n",
        "\n",
        "def get_gradcam(img,\n",
        "                score,\n",
        "                cnn_model,\n",
        "                model_modifier=model_modifier_function):\n",
        "\n",
        "    # Create Gradcam object\n",
        "    gradcam = Gradcam(cnn_model,\n",
        "                      model_modifier,\n",
        "                      clone=True)\n",
        "\n",
        "    # Generate heatmap with GradCAM\n",
        "    cam = gradcam(score,\n",
        "                  img,\n",
        "                  seek_penultimate_conv_layer=True)\n",
        "\n",
        "    heatmap = np.uint8(cm.jet(cam[0])[..., :3] * 255)\n",
        "\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "def get_gradcam_plus(img,\n",
        "                    score,\n",
        "                    model,\n",
        "                    model_modifier=ReplaceToLinear()):\n",
        "\n",
        "    # Create GradCAM++ object\n",
        "    gradcam = GradcamPlusPlus(model,\n",
        "                          model_modifier=model_modifier,\n",
        "                          clone=True)\n",
        "\n",
        "    cam = gradcam(score,\n",
        "                  img)\n",
        "\n",
        "    heatmap = np.uint8(cm.jet(cam[0])[..., :3] * 255)\n",
        "\n",
        "    return heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T12:02:27.268686Z",
          "iopub.status.busy": "2022-09-26T12:02:27.268354Z",
          "iopub.status.idle": "2022-09-26T12:02:28.502804Z",
          "shell.execute_reply": "2022-09-26T12:02:28.501629Z",
          "shell.execute_reply.started": "2022-09-26T12:02:27.268648Z"
        },
        "id": "MnFTGSNHNZP9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "SAR_CNN = tf.keras.models.load_model('CNN_models/SAR_CNN.h5',\n",
        "                                     custom_objects={'f1_score':f1_score,\n",
        "                                                     'recall_m':recall_m,\n",
        "                                                     'precision_m':precision_m}\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T12:02:29.561223Z",
          "iopub.status.busy": "2022-09-26T12:02:29.560739Z",
          "iopub.status.idle": "2022-09-26T12:02:58.372441Z",
          "shell.execute_reply": "2022-09-26T12:02:58.371448Z",
          "shell.execute_reply.started": "2022-09-26T12:02:29.56119Z"
        },
        "id": "Hb0Br6boNZP9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.subplots(4,4,figsize=(8*3,8*3))\n",
        "n = 4\n",
        "idx= 1\n",
        "\n",
        "\n",
        "for images,labels in S1_dataset_val.shuffle(buffer_size=5).take(1):\n",
        "    print(images.shape,labels.shape)\n",
        "\n",
        "    for i in range(4):\n",
        "        #get label\n",
        "        img = images[i]\n",
        "        lab = int(labels[i].numpy())\n",
        "\n",
        "        #predict on image\n",
        "        pred = np.argmax(SAR_CNN.predict(img[tf.newaxis,:,:,:]))\n",
        "        prd = int(pred.ravel())\n",
        "\n",
        "        score1 = CategoricalScore(lab)\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[:,:,0])\n",
        "        idx+=1\n",
        "\n",
        "        #saliency\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]} (saliency map)')\n",
        "        sal = get_saliency(img,\n",
        "                           score1,\n",
        "                           cnn_model = SAR_CNN).squeeze(axis=0)\n",
        "\n",
        "#         print(sal.shape)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[:,:,0])\n",
        "        plt.imshow(sal,alpha=0.25,cmap='jet') #overlay\n",
        "        idx+=1\n",
        "\n",
        "        #gradcam\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam = get_gradcam(img,\n",
        "                            score1,\n",
        "                           cnn_model = SAR_CNN)\n",
        "        plt.imshow(img[:,:,0])\n",
        "        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "\n",
        "        #gradcam ++\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam_pls = get_gradcam_plus(img,\n",
        "                                     score1,\n",
        "                                     model = SAR_CNN)\n",
        "        plt.imshow(img[:,:,0])\n",
        "        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "        if idx>16:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-09-26T12:02:58.374717Z",
          "iopub.status.busy": "2022-09-26T12:02:58.374269Z",
          "iopub.status.idle": "2022-09-26T12:03:26.310651Z",
          "shell.execute_reply": "2022-09-26T12:03:26.309648Z",
          "shell.execute_reply.started": "2022-09-26T12:02:58.374681Z"
        },
        "id": "3_Ar9EVLNZP9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.subplots(4,4,figsize=(8*3,8*3))\n",
        "n = 4\n",
        "idx= 1\n",
        "\n",
        "for images,labels in S1_dataset_val.shuffle(buffer_size=5).take(1):\n",
        "    print(images.shape,labels.shape)\n",
        "\n",
        "    for i in range(4):\n",
        "        #get label\n",
        "        img = images[i]\n",
        "        lab = int(labels[i].numpy())\n",
        "\n",
        "        #predict on image\n",
        "        pred = np.argmax(SAR_CNN.predict(img[tf.newaxis,:,:,:]))\n",
        "        score1 = CategoricalScore(lab)\n",
        "        prd = int(pred.ravel())\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[:,:,1])\n",
        "        idx+=1\n",
        "        #saliency\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]} (saliency map)')\n",
        "        sal = get_saliency(img,\n",
        "                           score1,\n",
        "                           cnn_model = SAR_CNN).squeeze(axis=0)\n",
        "\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[:,:,1])\n",
        "        plt.imshow(sal,alpha=0.25,cmap='jet') #overlay\n",
        "        idx+=1\n",
        "\n",
        "        #gradcam\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam = get_gradcam(img,\n",
        "                            score1,\n",
        "                           cnn_model = SAR_CNN)\n",
        "        plt.imshow(img[:,:,1])\n",
        "        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "\n",
        "        #gradcam ++\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam_pls = get_gradcam_plus(img,\n",
        "                                     score1,\n",
        "                                     model = SAR_CNN)\n",
        "        plt.imshow(img[:,:,1])\n",
        "        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "        if idx>16:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T12:03:26.313767Z",
          "iopub.status.busy": "2022-09-26T12:03:26.313193Z",
          "iopub.status.idle": "2022-09-26T12:03:26.680021Z",
          "shell.execute_reply": "2022-09-26T12:03:26.679224Z",
          "shell.execute_reply.started": "2022-09-26T12:03:26.31373Z"
        },
        "id": "xB6Oll2jNZP-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#delete\n",
        "del SAR_CNN;S1_dataset_val;S1_dataset_tr;gc.collect()\n",
        "\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0QzyoN0NZP-"
      },
      "source": [
        "# Building and training the RGB - CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T12:06:47.115896Z",
          "iopub.status.busy": "2022-09-26T12:06:47.115283Z",
          "iopub.status.idle": "2022-09-26T12:06:48.907061Z",
          "shell.execute_reply": "2022-09-26T12:06:48.906234Z",
          "shell.execute_reply.started": "2022-09-26T12:06:47.115851Z"
        },
        "id": "ebMR2P39NZP-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "RGB_CNN = multichannel_cnn(num_channels = 3,\n",
        "                           hidden_units = 512, #number of  hidden dense\n",
        "                           weights = 'imagenet'\n",
        "                          )\n",
        "\n",
        "\n",
        "RGB_CNN.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss = 'sparse_categorical_crossentropy',\n",
        "                metrics = ['accuracy',f1_score,recall_m,precision_m])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T12:06:51.782747Z",
          "iopub.status.busy": "2022-09-26T12:06:51.782268Z"
        },
        "id": "BR_JJaN0NZP-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "hist2 = RGB_CNN.fit(RGB_dataset_tr,\n",
        "                    validation_data = RGB_dataset_val,\n",
        "                    epochs = EPOCHS,\n",
        "                    callbacks = callbacks_1)\n",
        "\n",
        "#save model\n",
        "\n",
        "RGB_CNN.save(filepath = 'CNN_models/RGB_CNN.h5')\n",
        "\n",
        "plot_history(hist2,'f1_score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raDuGkfoNZP-"
      },
      "source": [
        "**Checking the GradCAM and saliency Maps for RGB CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2022-09-26T11:18:35.447909Z",
          "iopub.status.idle": "2022-09-26T11:18:35.44864Z",
          "shell.execute_reply": "2022-09-26T11:18:35.448374Z",
          "shell.execute_reply.started": "2022-09-26T11:18:35.448349Z"
        },
        "id": "p6tpCY-nNZP-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.subplots(4,4,figsize=(8*3,8*3))\n",
        "n = 4\n",
        "idx= 1\n",
        "\n",
        "\n",
        "for images,labels in RGB_dataset_val.shuffle(buffer_size=12).take(1):\n",
        "\n",
        "    for i in range(4):\n",
        "        #get label\n",
        "        img = images[i]\n",
        "        lab = int(labels[i].numpy())\n",
        "\n",
        "\n",
        "#         print(img.shape,lab.shape)\n",
        "        score1 = CategoricalScore(lab)\n",
        "\n",
        "\n",
        "\n",
        "        #predict on image\n",
        "        prd= np.argmax(RGB_CNN.predict(img[tf.newaxis,:,:,:]))\n",
        "\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        idx+=1\n",
        "\n",
        "        #saliency\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n",
        "        sal = get_saliency(img,\n",
        "                           score1,\n",
        "                           cnn_model = RGB_CNN).squeeze(axis=0)\n",
        "\n",
        "#         print(sal.shape)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(sal,alpha=0.45,cmap='jet') #overlay\n",
        "        idx+=1\n",
        "\n",
        "        #gradcam\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam = get_gradcam(img,\n",
        "                            score1,\n",
        "                           cnn_model = RGB_CNN)\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "\n",
        "        #gradcam ++\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam_pls = get_gradcam_plus(img,\n",
        "                                     score1,\n",
        "                                     model = RGB_CNN)\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "        if idx>16:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2022-09-26T11:18:35.449816Z",
          "iopub.status.idle": "2022-09-26T11:18:35.45048Z",
          "shell.execute_reply": "2022-09-26T11:18:35.450276Z",
          "shell.execute_reply.started": "2022-09-26T11:18:35.450252Z"
        },
        "id": "-LK4wvvTNZP_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.subplots(4,4,figsize=(8*3,8*3))\n",
        "n = 4\n",
        "idx= 1\n",
        "\n",
        "\n",
        "for images,labels in RGB_dataset_val.shuffle(buffer_size=12).take(1):\n",
        "\n",
        "    for i in range(4):\n",
        "        #get label\n",
        "        img = images[i]\n",
        "        lab = int(labels[i].numpy())\n",
        "\n",
        "\n",
        "#         print(img.shape,lab.shape)\n",
        "        score1 = CategoricalScore(lab)\n",
        "\n",
        "\n",
        "\n",
        "        #predict on image\n",
        "        prd= np.argmax(RGB_CNN.predict(img[tf.newaxis,:,:,:]))\n",
        "\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        idx+=1\n",
        "\n",
        "        #saliency\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n",
        "        sal = get_saliency(img,\n",
        "                           score1,\n",
        "                           cnn_model = RGB_CNN).squeeze(axis=0)\n",
        "\n",
        "#         print(sal.shape)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(sal,alpha=0.45,cmap='jet') #overlay\n",
        "        idx+=1\n",
        "\n",
        "        #gradcam\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam = get_gradcam(img,\n",
        "                            score1,\n",
        "                           cnn_model = RGB_CNN)\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "\n",
        "        #gradcam ++\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam_pls = get_gradcam_plus(img,\n",
        "                                     score1,\n",
        "                                     model = RGB_CNN)\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "        if idx>16:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-26T11:18:35.451633Z",
          "iopub.status.idle": "2022-09-26T11:18:35.452291Z",
          "shell.execute_reply": "2022-09-26T11:18:35.452074Z",
          "shell.execute_reply.started": "2022-09-26T11:18:35.452051Z"
        },
        "id": "UjZZNvkdNZP_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#delete\n",
        "del RGB_CNN;gc.collect()\n",
        "\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NQN8yYpNZP_"
      },
      "source": [
        "# Building and training the Multispectral - CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2022-09-26T11:18:35.453538Z",
          "iopub.status.idle": "2022-09-26T11:18:35.454256Z",
          "shell.execute_reply": "2022-09-26T11:18:35.454027Z",
          "shell.execute_reply.started": "2022-09-26T11:18:35.454002Z"
        },
        "id": "GU0zaJ_yNZP_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "multispectral_CNN = multichannel_cnn(num_channels = 12,\n",
        "                                    hidden_units = 512, #number of  hidden dense\n",
        "                                    )\n",
        "\n",
        "\n",
        "#check on some data\n",
        "# multispectral_CNN(x)\n",
        "\n",
        "\n",
        "multispectral_CNN.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001,momentum = 0.0),\n",
        "                loss = 'sparse_categorical_crossentropy',\n",
        "                metrics = ['accuracy',f1_score,recall_m,precision_m])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2022-09-26T11:18:35.455455Z",
          "iopub.status.idle": "2022-09-26T11:18:35.456181Z",
          "shell.execute_reply": "2022-09-26T11:18:35.455953Z",
          "shell.execute_reply.started": "2022-09-26T11:18:35.455927Z"
        },
        "id": "FA9aTyHENZP_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "hist2 = multispectral_CNN.fit(S2_dataset_tr,\n",
        "                    validation_data = S2_dataset_val,\n",
        "                    epochs = EPOCHS,\n",
        "                    callbacks = callbacks_1)\n",
        "\n",
        "#save model\n",
        "\n",
        "multispectral_CNN.save(filepath = 'CNN_models/S2_CNN.h5')\n",
        "\n",
        "plot_history(hist2,'f1_score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg0B0Q8gNZQA"
      },
      "source": [
        "**Checking the decision mechanism for Multispectral model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2022-09-26T11:18:35.457484Z",
          "iopub.status.idle": "2022-09-26T11:18:35.458198Z",
          "shell.execute_reply": "2022-09-26T11:18:35.457971Z",
          "shell.execute_reply.started": "2022-09-26T11:18:35.457945Z"
        },
        "id": "06HltOh1NZQA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.subplots(4,4,figsize=(8*3,8*3))\n",
        "n = 4\n",
        "idx= 1\n",
        "\n",
        "\n",
        "for images,labels in S2_dataset_val.shuffle(buffer_size=12).take(1):\n",
        "    print(images.shape,labels.shape)\n",
        "\n",
        "    for i in range(4):\n",
        "        #get label\n",
        "        img = images[i]\n",
        "        lab = int(labels[i].numpy())\n",
        "\n",
        "\n",
        "#         print(img.shape,lab.shape)\n",
        "        score1 = CategoricalScore(lab)\n",
        "\n",
        "\n",
        "\n",
        "        #predict on image\n",
        "        prd= np.argmax(multispectral_CNN.predict(img[tf.newaxis,:,:,:]))\n",
        "\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[:,:,1:4][:,:,::-1])\n",
        "        idx+=1\n",
        "\n",
        "        #saliency\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n",
        "        sal = get_saliency(img,\n",
        "                           score1,\n",
        "                           cnn_model = multispectral_CNN).squeeze(axis=0)\n",
        "\n",
        "#         print(sal.shape)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[:,:,1:4][:,:,::-1])\n",
        "        plt.imshow(sal,alpha=0.45,cmap='jet') #overlay\n",
        "        idx+=1\n",
        "\n",
        "        #gradcam\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam = get_gradcam(img,\n",
        "                            score1,\n",
        "                           cnn_model = multispectral_CNN)\n",
        "        plt.imshow(img[:,:,1:4][:,:,::-1])\n",
        "        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "\n",
        "        #gradcam ++\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam_pls = get_gradcam_plus(img,\n",
        "                                     score1,\n",
        "                                     model = multispectral_CNN)\n",
        "        plt.imshow(img[:,:,1:4][:,:,::-1])\n",
        "        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "        if idx>16:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2022-09-26T11:18:35.459419Z",
          "iopub.status.idle": "2022-09-26T11:18:35.460084Z",
          "shell.execute_reply": "2022-09-26T11:18:35.45988Z",
          "shell.execute_reply.started": "2022-09-26T11:18:35.459856Z"
        },
        "id": "O7xJSNntNZQA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.subplots(4,4,figsize=(8*3,8*3))\n",
        "n = 4\n",
        "idx= 1\n",
        "\n",
        "\n",
        "for images,labels in S2_dataset_val.shuffle(buffer_size=12).take(1):\n",
        "    print(images.shape,labels.shape)\n",
        "\n",
        "    for i in range(4):\n",
        "        #get label\n",
        "        img = images[i]\n",
        "        lab = int(labels[i].numpy())\n",
        "\n",
        "\n",
        "#         print(img.shape,lab.shape)\n",
        "        score1 = CategoricalScore(lab)\n",
        "\n",
        "\n",
        "\n",
        "        #predict on image\n",
        "        prd= np.argmax(multispectral_CNN.predict(img[tf.newaxis,:,:,:]))\n",
        "\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[:,:,1:4][:,:,::-1])\n",
        "        idx+=1\n",
        "\n",
        "        #saliency\n",
        "\n",
        "        plt.subplot(4,4,idx)\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n",
        "        sal = get_saliency(img,\n",
        "                           score1,\n",
        "                           cnn_model = multispectral_CNN).squeeze(axis=0)\n",
        "\n",
        "#         print(sal.shape)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[:,:,1:4][:,:,::-1])\n",
        "        plt.imshow(sal,alpha=0.30,cmap='jet') #overlay\n",
        "        idx+=1\n",
        "\n",
        "        #gradcam\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam = get_gradcam(img,\n",
        "                            score1,\n",
        "                           cnn_model = multispectral_CNN)\n",
        "        plt.imshow(img[:,:,1:4][:,:,::-1])\n",
        "        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "\n",
        "        #gradcam ++\n",
        "        plt.subplot(4,4,idx)\n",
        "        gdcam_pls = get_gradcam_plus(img,\n",
        "                                     score1,\n",
        "                                     model = multispectral_CNN)\n",
        "        plt.imshow(img[:,:,1:4][:,:,::-1])\n",
        "        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n",
        "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n",
        "        plt.axis('off')\n",
        "        idx+=1\n",
        "\n",
        "        if idx>16:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9xDmM-PNZQB"
      },
      "source": [
        "# Model for temporal forecasting\n",
        "\n",
        "    This notebooks covered the prediction of flooding using satellite images. But this dataset is as time series dataset, which has multiple images of locations before and during the flood. So we will try to cover the temporal aspect of predicting flooding in the next notebook. This can be especially important in some cases where there are water bodies in the image, and hence there needs to be additional signal to distinguish cases of flooding ffrom normal conditions\n",
        "    \n",
        "[The temporal forecasting notebook can be found here](https://www.kaggle.com/code/virajkadam/detecting-floods-time-series-prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqtREKR5NZQB"
      },
      "source": [
        "# References and resources\n",
        "\n",
        "* https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B2-2020/1343/2020/isprs-archives-XLIII-B2-2020-1343-2020.pdf\n",
        "* https://arxiv.org/pdf/2006.10027v1.pdf#page=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAtNKP1Kt6O7",
        "outputId": "c0aa3a86-b8f7-462b-a438-2c90b37c852f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentinelhub\n",
            "  Downloading sentinelhub-3.11.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aenum>=2.1.4 (from sentinelhub)\n",
            "  Downloading aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (8.1.8)\n",
            "Collecting dataclasses-json (from sentinelhub)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (2.0.2)\n",
            "Requirement already satisfied: oauthlib in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (3.2.2)\n",
            "Requirement already satisfied: pillow>=9.2.0 in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (11.1.0)\n",
            "Requirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (2.8.2)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (2.0.0)\n",
            "Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (2.32.3)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (2.0.7)\n",
            "Requirement already satisfied: tifffile>=2020.9.30 in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (2025.3.13)\n",
            "Collecting tomli (from sentinelhub)\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tomli-w (from sentinelhub)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentinelhub) (4.12.2)\n",
            "Collecting utm (from sentinelhub)\n",
            "  Downloading utm-0.8.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=2.2.0->sentinelhub) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.0->sentinelhub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.0->sentinelhub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.0->sentinelhub) (2.3.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->sentinelhub)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->sentinelhub)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->sentinelhub) (1.17.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->sentinelhub) (24.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->sentinelhub)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading sentinelhub-3.11.1-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.8/249.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading utm-0.8.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: aenum, utm, tomli-w, tomli, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, sentinelhub\n",
            "Successfully installed aenum-3.1.15 dataclasses-json-0.6.7 marshmallow-3.26.1 mypy-extensions-1.0.0 sentinelhub-3.11.1 tomli-2.2.1 tomli-w-1.2.0 typing-inspect-0.9.0 utm-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentinelhub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wRJRuZrE6hO"
      },
      "source": [
        "# **maps**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vCzkmq_6voNi",
        "outputId": "8f8e5f99-b188-4562-9909-c0a96faa734b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <style>\n",
              "                .geemap-dark {\n",
              "                    --jp-widgets-color: white;\n",
              "                    --jp-widgets-label-color: white;\n",
              "                    --jp-ui-font-color1: white;\n",
              "                    --jp-layout-color2: #454545;\n",
              "                    background-color: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-dark .jupyter-button {\n",
              "                    --jp-layout-color3: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-colab {\n",
              "                    background-color: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "\n",
              "                .geemap-colab .jupyter-button {\n",
              "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "            </style>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image downloaded as satellite_image.png\n"
          ]
        }
      ],
      "source": [
        "from sentinelhub import SentinelHubRequest, MimeType, CRS, BBox, bbox_to_dimensions, DataCollection\n",
        "\n",
        "# Define latitude & longitude\n",
        "latitude = 26.489679   # Change to your location\n",
        "longitude = 74.550941\n",
        "\n",
        "# Define bounding box (small area)\n",
        "bbox = BBox([longitude - 0.01, latitude - 0.01, longitude + 0.01, latitude + 0.01], CRS.WGS84)\n",
        "\n",
        "# Define image resolution\n",
        "resolution = 2   # 10m per pixel\n",
        "\n",
        "# Get dimensions\n",
        "size = bbox_to_dimensions(bbox, resolution=resolution)\n",
        "\n",
        "# Create request for Sentinel-2 image\n",
        "request = SentinelHubRequest(\n",
        "    evalscript=\"\"\"\n",
        "    // Extract RGB image\n",
        "    function setup() {\n",
        "        return {\n",
        "            input: [\"B04\", \"B03\", \"B02\"],\n",
        "            output: { bands: 3 }\n",
        "        };\n",
        "    }\n",
        "    function evaluatePixel(sample) {\n",
        "        return [sample.B04, sample.B03, sample.B02];\n",
        "    }\n",
        "    \"\"\",\n",
        "    input_data=[SentinelHubRequest.input_data(DataCollection.SENTINEL2_L2A, maxcc=0.1, time_interval=(\"2024-01-01\", \"2024-12-31\"))],  # Fixed maxcc\n",
        "    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n",
        "    bbox=bbox,\n",
        "    size=size\n",
        ")\n",
        "\n",
        "# Get image\n",
        "image = request.get_data()[0]\n",
        "\n",
        "# Save image\n",
        "import imageio\n",
        "imageio.imwrite(\"satellite_image.png\", image)\n",
        "\n",
        "print(\"Image downloaded as satellite_image.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oGmg0qxdxDB9"
      },
      "outputs": [],
      "source": [
        "from sentinelhub import SHConfig\n",
        "\n",
        "config = SHConfig()\n",
        "config.sh_client_id = \"7a3098a7-bfcd-4bff-baa4-6104fef14169\"\n",
        "config.sh_client_secret = \"bTOS03EAVbfImGlrGqHjMkstr9IJ0aQ1\"\n",
        "config.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "ejy9ZVOwHCZT",
        "outputId": "734a2247-5210-4a07-a6cf-3998da2ff233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved flood_risk_9984.png\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m model_sar = tf.keras.models.load_model(\u001b[33m\"\u001b[39m\u001b[33m/home/arshlaan/projext/codathon/SAR_CNN.h5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Upload your image file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m uploaded_img = \u001b[43mfiles\u001b[49m.upload()  \u001b[38;5;66;03m# When prompted, select your .tif image file\u001b[39;00m\n\u001b[32m     62\u001b[39m image_path = \u001b[38;5;28mlist\u001b[39m(uploaded_img.keys())[\u001b[32m0\u001b[39m]\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUploaded image file:\u001b[39m\u001b[33m\"\u001b[39m, image_path)\n",
            "\u001b[31mNameError\u001b[39m: name 'files' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sentinelhub import SHConfig, BBox, CRS, SentinelHubRequest, DataCollection, MimeType\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/home/arshlaan/projext/codathon/flood-risk-in-india/flood_risk_dataset_india.csv\")  # Update with the correct file name\n",
        "\n",
        "# Sort the dataset by flood probability in descending order\n",
        "df_sorted = df.sort_values(by=\"Flood Occurred\", ascending=False)\n",
        "\n",
        "\n",
        "# Function to get and save an image for a given latitude and longitude\n",
        "def download_image(lat, lon, index):\n",
        "    bbox = BBox(((lon - 0.01, lat - 0.01), (lon + 0.01, lat + 0.01)), crs=CRS.WGS84)\n",
        "\n",
        "    request = SentinelHubRequest(\n",
        "        evalscript=\"\"\"\n",
        "        // Simple true color script\n",
        "        function setup() {\n",
        "            return {\n",
        "                input: [\"B04\", \"B03\", \"B02\"],\n",
        "                output: { bands: 3 }\n",
        "            };\n",
        "        }\n",
        "        function evaluatePixel(sample) {\n",
        "            return [sample.B04, sample.B03, sample.B02];\n",
        "        }\n",
        "        \"\"\",\n",
        "        input_data=[\n",
        "            SentinelHubRequest.input_data(\n",
        "                DataCollection.SENTINEL2_L2A,\n",
        "                time_interval=(\"2024-01-01\", \"2024-12-31\")\n",
        "            )\n",
        "        ],\n",
        "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n",
        "        bbox=bbox,\n",
        "        size=(512, 512),\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    image = request.get_data()[0]\n",
        "    file_name = f\"flood_risk_{index}.png\"\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        file.write(image.tobytes())\n",
        "    print(f\"Saved {file_name}\")\n",
        "\n",
        "# Process and download images for the sorted locations\n",
        "for idx, row in df_sorted.iterrows():\n",
        "    download_image(row[\"Latitude\"], row[\"Longitude\"], idx)\n",
        "    import tensorflow as tf\n",
        "    import numpy as np\n",
        "    import cv2\n",
        "    from PIL import Image\n",
        "    import rasterio\n",
        "\n",
        "    # Load models (using compile=False to avoid optimizer warnings)\n",
        "    model_rgb = tf.keras.models.load_model(\"/home/arshlaan/projext/codathon/RGB_CNN.h5\", compile=False)\n",
        "    model_s2  = tf.keras.models.load_model(\"/home/arshlaan/projext/codathon/S2_CNN.h5\", compile=False)\n",
        "    model_sar = tf.keras.models.load_model(\"/home/arshlaan/projext/codathon/SAR_CNN.h5\", compile=False)\n",
        "\n",
        "    # Upload your image file\n",
        "    uploaded_img = files.upload()  # When prompted, select your .tif image file\n",
        "    image_path = list(uploaded_img.keys())[0]\n",
        "    print(\"Uploaded image file:\", image_path)\n",
        "\n",
        "    # Load the image using rasterio if it's a .tif\n",
        "    if image_path.lower().endswith('.tif'):\n",
        "        with rasterio.open(image_path) as src:\n",
        "            image_array = src.read()  # shape: (channels, height, width)\n",
        "        print(\"Original shape from rasterio:\", image_array.shape)\n",
        "\n",
        "        # Handle channels for S2 model:\n",
        "        if image_array.shape[0] == 1:\n",
        "            # Duplicate the single channel 12 times\n",
        "            image_array = np.repeat(image_array, 12, axis=0)\n",
        "            print(\"Duplicated single channel to 12 channels:\", image_array.shape)\n",
        "        elif image_array.shape[0] < 12:\n",
        "            padding_channels = 12 - image_array.shape[0]\n",
        "            image_array = np.pad(image_array, ((0, padding_channels), (0,0), (0,0)), mode='constant')\n",
        "            print(\"Padded to 12 channels:\", image_array.shape)\n",
        "        elif image_array.shape[0] > 12:\n",
        "            image_array = image_array[:12, :, :]\n",
        "            print(\"Using the first 12 channels:\", image_array.shape)\n",
        "\n",
        "        # Convert from (channels, height, width) to (height, width, channels)\n",
        "        image_array = np.moveaxis(image_array, 0, -1)\n",
        "    else:\n",
        "        # For non-TIFF images, load with PIL and convert to RGB, then pad to 12 channels if needed\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image_array = np.array(image)\n",
        "        if image_array.shape[-1] == 3:\n",
        "            padding_channels = 12 - 3\n",
        "            image_array = np.concatenate([image_array, np.zeros((image_array.shape[0], image_array.shape[1], padding_channels), dtype=image_array.dtype)], axis=-1)\n",
        "            print(\"Converted non-TIFF image to 12 channels:\", image_array.shape)\n",
        "\n",
        "    print(\"Shape after processing:\", image_array.shape)\n",
        "\n",
        "    # Resize the image to 256x256 (model's expected input size)\n",
        "    image_resized = cv2.resize(image_array, (256, 256))\n",
        "    print(\"Shape after resizing:\", image_resized.shape)\n",
        "\n",
        "    # Normalize the image (assuming pixel values range 0-255)\n",
        "    image_normalized = image_resized / 255.0\n",
        "\n",
        "    # Convert to tensor and add a batch dimension: expected shape (1, 256, 256, 12)\n",
        "    tensor_image = tf.convert_to_tensor(image_normalized, dtype=tf.float32)\n",
        "    tensor_image = tf.expand_dims(tensor_image, axis=0)\n",
        "    print(\"Final tensor shape:\", tensor_image.shape)\n",
        "\n",
        "    # Prepare inputs for each model:\n",
        "    # For the RGB model, use the first 3 channels.\n",
        "    tensor_image_rgb = tensor_image[..., :3]\n",
        "    # For the S2 model, use all 12 channels.\n",
        "    tensor_image_s2 = tensor_image\n",
        "    # For the SAR model, use the first 2 channels.\n",
        "    tensor_image_sar = tensor_image[..., :2]\n",
        "\n",
        "    print(\"RGB input shape:\", tensor_image_rgb.shape)\n",
        "    print(\"S2 input shape:\", tensor_image_s2.shape)\n",
        "    print(\"SAR input shape:\", tensor_image_sar.shape)\n",
        "\n",
        "    # Run predictions from each model\n",
        "    pred_rgb = model_rgb.predict(tensor_image_rgb)\n",
        "    pred_s2  = model_s2.predict(tensor_image_s2)\n",
        "    pred_sar = model_sar.predict(tensor_image_sar)\n",
        "\n",
        "    # Average the predictions for an ensemble output\n",
        "    avg_pred = np.mean([pred_rgb, pred_s2, pred_sar], axis=0)\n",
        "\n",
        "    print(\"RGB Model Prediction:\", pred_rgb)\n",
        "    print(\"S2 Model Prediction:\", pred_s2)\n",
        "    print(\"SAR Model Prediction:\", pred_sar)\n",
        "    print(\"Averaged Prediction:\", avg_pred)\n",
        "    # Assuming avg_pred is [[prob_no_flood, prob_flood]]\n",
        "    prob_no_flood = avg_pred[0][0] * 100\n",
        "    prob_flood = avg_pred[0][1] * 100\n",
        "\n",
        "    # Print predictions in percentage\n",
        "    print(f\"Probability of No Flooding: {prob_no_flood:.2f}%\")\n",
        "    print(f\"Probability of Flooding: {prob_flood:.2f}%\")\n",
        "\n",
        "    # Determine and print a message based on which probability is higher\n",
        "    if prob_flood > prob_no_flood:\n",
        "        print(f\"Flooding is likely with a {prob_flood:.2f}% probability.\")\n",
        "    else:\n",
        "        print(f\"No flooding is detected with a {prob_no_flood:.2f}% probability.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved /home/arshlaan/projext/codathon/flood_risk_9984.png\n",
            "Processing image: /home/arshlaan/projext/codathon/flood_risk_9984.png\n"
          ]
        },
        {
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file '/home/arshlaan/projext/codathon/flood_risk_9984.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnidentifiedImageError\u001b[39m                    Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df_sorted.iterrows():\n\u001b[32m    163\u001b[39m     image_path = download_image(row[\u001b[33m\"\u001b[39m\u001b[33mLatitude\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mLongitude\u001b[39m\u001b[33m\"\u001b[39m], idx)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[43mprocess_and_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mprocess_and_predict\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m    103\u001b[39m     image_array = np.moveaxis(image_array, \u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# For non-TIFF images, load using PIL\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    107\u001b[39m     image_array = np.array(image)\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m image_array.shape[-\u001b[32m1\u001b[39m] == \u001b[32m3\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/PIL/Image.py:3532\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3530\u001b[39m     warnings.warn(message)\n\u001b[32m   3531\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[32m-> \u001b[39m\u001b[32m3532\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
            "\u001b[31mUnidentifiedImageError\u001b[39m: cannot identify image file '/home/arshlaan/projext/codathon/flood_risk_9984.png'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sentinelhub import SHConfig, BBox, CRS, SentinelHubRequest, DataCollection, MimeType\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import rasterio\n",
        "import os\n",
        "\n",
        "# --- Configuration and Dataset Loading ---\n",
        "\n",
        "# # Set up Sentinel Hub API credentials (update with your actual credentials)\n",
        "# config = SHConfig()\n",
        "# config.sh_client_id = \"YOUR_CLIENT_ID\"\n",
        "# config.sh_client_secret = \"YOUR_CLIENT_SECRET\"\n",
        "\n",
        "# Load the dataset from a local CSV file\n",
        "dataset_path = \"/home/arshlaan/projext/codathon/flood-risk-in-india/flood_risk_dataset_india.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Sort the dataset by flood probability (assumed column \"Flood Occurred\") in descending order\n",
        "df_sorted = df.sort_values(by=\"Flood Occurred\", ascending=False)\n",
        "\n",
        "# Load pre-trained models (update paths as necessary)\n",
        "model_rgb = tf.keras.models.load_model(\"/home/arshlaan/projext/codathon/RGB_CNN.h5\", compile=False)\n",
        "model_s2  = tf.keras.models.load_model(\"/home/arshlaan/projext/codathon/S2_CNN.h5\", compile=False)\n",
        "model_sar = tf.keras.models.load_model(\"/home/arshlaan/projext/codathon/SAR_CNN.h5\", compile=False)\n",
        "\n",
        "# --- Functions ---\n",
        "\n",
        "def download_image(lat, lon, index):\n",
        "    \"\"\"\n",
        "    Download a satellite image from Sentinel Hub for the specified latitude and longitude,\n",
        "    and save it to a designated directory.\n",
        "    \"\"\"\n",
        "    save_dir = \"/home/arshlaan/projext/codathon/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Create a bounding box around the coordinate (0.01 degrees offset)\n",
        "    bbox = BBox(((lon - 0.01, lat - 0.01), (lon + 0.01, lat + 0.01)), crs=CRS.WGS84)\n",
        "    \n",
        "    request = SentinelHubRequest(\n",
        "        evalscript=\"\"\"\n",
        "            // Simple true color script\n",
        "            function setup() {\n",
        "                return {\n",
        "                    input: [\"B04\", \"B03\", \"B02\"],\n",
        "                    output: { bands: 3 }\n",
        "                };\n",
        "            }\n",
        "            function evaluatePixel(sample) {\n",
        "                return [sample.B04, sample.B03, sample.B02];\n",
        "            }\n",
        "        \"\"\",\n",
        "        input_data=[\n",
        "            SentinelHubRequest.input_data(\n",
        "                DataCollection.SENTINEL2_L2A,\n",
        "                time_interval=(\"2024-01-01\", \"2024-12-31\")\n",
        "            )\n",
        "        ],\n",
        "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n",
        "        bbox=bbox,\n",
        "        size=(512, 512),\n",
        "        config=config\n",
        "    )\n",
        "    \n",
        "    # Request the image from Sentinel Hub\n",
        "    image = request.get_data()[0]\n",
        "    file_name = os.path.join(save_dir, f\"flood_risk_{index}.png\")\n",
        "    \n",
        "    # Save the image file locally\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        file.write(image.tobytes())\n",
        "    print(f\"Saved {file_name}\")\n",
        "    return file_name\n",
        "\n",
        "def process_and_predict(image_path):\n",
        "    \"\"\"\n",
        "    Process the downloaded image (handling TIFF and non-TIFF cases),\n",
        "    resize and normalize it, then run predictions using three models.\n",
        "    \"\"\"\n",
        "    print(\"Processing image:\", image_path)\n",
        "    \n",
        "    # Check if the file is a TIFF image\n",
        "    if image_path.lower().endswith('.tif'):\n",
        "        with rasterio.open(image_path) as src:\n",
        "            image_array = src.read()  # shape: (channels, height, width)\n",
        "        print(\"Original shape from rasterio:\", image_array.shape)\n",
        "        \n",
        "        # Adjust channels to ensure we have 12 channels for the S2 model:\n",
        "        if image_array.shape[0] == 1:\n",
        "            image_array = np.repeat(image_array, 12, axis=0)\n",
        "            print(\"Duplicated single channel to 12 channels:\", image_array.shape)\n",
        "        elif image_array.shape[0] < 12:\n",
        "            padding_channels = 12 - image_array.shape[0]\n",
        "            image_array = np.pad(image_array, ((0, padding_channels), (0,0), (0,0)), mode='constant')\n",
        "            print(\"Padded to 12 channels:\", image_array.shape)\n",
        "        elif image_array.shape[0] > 12:\n",
        "            image_array = image_array[:12, :, :]\n",
        "            print(\"Using the first 12 channels:\", image_array.shape)\n",
        "        \n",
        "        # Convert from (channels, height, width) to (height, width, channels)\n",
        "        image_array = np.moveaxis(image_array, 0, -1)\n",
        "    else:\n",
        "        # For non-TIFF images, load using PIL\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image_array = np.array(image)\n",
        "        if image_array.shape[-1] == 3:\n",
        "            padding_channels = 12 - 3\n",
        "            image_array = np.concatenate([image_array, np.zeros((image_array.shape[0], image_array.shape[1], padding_channels), dtype=image_array.dtype)], axis=-1)\n",
        "            print(\"Converted non-TIFF image to 12 channels:\", image_array.shape)\n",
        "    \n",
        "    print(\"Shape after processing:\", image_array.shape)\n",
        "    \n",
        "    # Resize the image to the model's expected input size (256x256)\n",
        "    image_resized = cv2.resize(image_array, (256, 256))\n",
        "    print(\"Shape after resizing:\", image_resized.shape)\n",
        "    \n",
        "    # Normalize pixel values to [0, 1]\n",
        "    image_normalized = image_resized / 255.0\n",
        "    \n",
        "    # Convert to a TensorFlow tensor and add a batch dimension (1, 256, 256, 12)\n",
        "    tensor_image = tf.convert_to_tensor(image_normalized, dtype=tf.float32)\n",
        "    tensor_image = tf.expand_dims(tensor_image, axis=0)\n",
        "    print(\"Final tensor shape:\", tensor_image.shape)\n",
        "    \n",
        "    # Prepare model inputs:\n",
        "    tensor_image_rgb = tensor_image[..., :3]  # For the RGB model\n",
        "    tensor_image_s2 = tensor_image           # For the S2 model (all 12 channels)\n",
        "    tensor_image_sar = tensor_image[..., :2]   # For the SAR model (first 2 channels)\n",
        "    \n",
        "    print(\"RGB input shape:\", tensor_image_rgb.shape)\n",
        "    print(\"S2 input shape:\", tensor_image_s2.shape)\n",
        "    print(\"SAR input shape:\", tensor_image_sar.shape)\n",
        "    \n",
        "    # Run predictions for each model\n",
        "    pred_rgb = model_rgb.predict(tensor_image_rgb)\n",
        "    pred_s2  = model_s2.predict(tensor_image_s2)\n",
        "    pred_sar = model_sar.predict(tensor_image_sar)\n",
        "    \n",
        "    # Average the predictions (ensemble)\n",
        "    avg_pred = np.mean([pred_rgb, pred_s2, pred_sar], axis=0)\n",
        "    print(\"RGB Model Prediction:\", pred_rgb)\n",
        "    print(\"S2 Model Prediction:\", pred_s2)\n",
        "    print(\"SAR Model Prediction:\", pred_sar)\n",
        "    print(\"Averaged Prediction:\", avg_pred)\n",
        "    \n",
        "    # Calculate probabilities (assuming avg_pred format: [[prob_no_flood, prob_flood]])\n",
        "    prob_no_flood = avg_pred[0][0] * 100\n",
        "    prob_flood = avg_pred[0][1] * 100\n",
        "    \n",
        "    print(f\"Probability of No Flooding: {prob_no_flood:.2f}%\")\n",
        "    print(f\"Probability of Flooding: {prob_flood:.2f}%\")\n",
        "    \n",
        "    if prob_flood > prob_no_flood:\n",
        "        print(f\"Flooding is likely with a {prob_flood:.2f}% probability.\")\n",
        "    else:\n",
        "        print(f\"No flooding is detected with a {prob_no_flood:.2f}% probability.\")\n",
        "\n",
        "# --- Main Loop ---\n",
        "# For each row in the sorted dataset, download the image and process it.\n",
        "for idx, row in df_sorted.iterrows():\n",
        "    image_path = download_image(row[\"Latitude\"], row[\"Longitude\"], idx)\n",
        "    process_and_predict(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV8U2xnIHIOd",
        "outputId": "d2497f2f-9308-4dc4-fbe3-9be91a01893b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.11/dist-packages (0.1.22)\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.1.31)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.1.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDr1YQjwHGoK",
        "outputId": "c862ae57-22a5-4556-ec76-ad641a56cb5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/s3programmer/flood-risk-in-india\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/s3programmer/flood-risk-in-india/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/arshlaan/projext/codathon/fld/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image downloaded as satellite_image.png\n"
          ]
        }
      ],
      "source": [
        "from sentinelhub import SentinelHubRequest, MimeType, CRS, BBox, bbox_to_dimensions, DataCollection\n",
        "\n",
        "# Define latitude & longitude\n",
        "latitude = 26.489679   # Change to your location\n",
        "longitude = 74.550941\n",
        "\n",
        "# Define bounding box (small area)\n",
        "bbox = BBox([longitude - 0.01, latitude - 0.01, longitude + 0.01, latitude + 0.01], CRS.WGS84)\n",
        "\n",
        "# Define image resolution\n",
        "resolution = 2   # 10m per pixel\n",
        "\n",
        "# Get dimensions\n",
        "size = bbox_to_dimensions(bbox, resolution=resolution)\n",
        "\n",
        "# Create request for Sentinel-2 image\n",
        "request = SentinelHubRequest(\n",
        "    evalscript=\"\"\"\n",
        "    // Extract RGB image\n",
        "    function setup() {\n",
        "        return {\n",
        "            input: [\"B04\", \"B03\", \"B02\"],\n",
        "            output: { bands: 3 }\n",
        "        };\n",
        "    }\n",
        "    function evaluatePixel(sample) {\n",
        "        return [sample.B04, sample.B03, sample.B02];\n",
        "    }\n",
        "    \"\"\",\n",
        "    input_data=[SentinelHubRequest.input_data(DataCollection.SENTINEL2_L2A, maxcc=0.1, time_interval=(\"2024-01-01\", \"2024-12-31\"))],  # Fixed maxcc\n",
        "    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n",
        "    bbox=bbox,\n",
        "    size=size\n",
        ")\n",
        "\n",
        "# Get image\n",
        "image = request.get_data()[0]\n",
        "\n",
        "# Save image\n",
        "import imageio\n",
        "imageio.imwrite(\"satellite_image.png\", image)\n",
        "\n",
        "print(\"Image downloaded as satellite_image.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image downloaded as satellite_image.png\n",
            "Processing image file: /home/arshlaan/projext/codathon/satellite_image.png\n",
            "Converted non-TIFF image to 12 channels: (1135, 902, 12)\n",
            "Shape after processing: (1135, 902, 12)\n",
            "Shape after resizing: (256, 256, 12)\n",
            "Final tensor shape: (1, 256, 256, 12)\n",
            "RGB input shape: (1, 256, 256, 3)\n",
            "S2 input shape: (1, 256, 256, 12)\n",
            "SAR input shape: (1, 256, 256, 2)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f887dc0ff60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f887dc0fc40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "RGB Model Prediction: [[3.338135e-05 9.999666e-01]]\n",
            "S2 Model Prediction: [[1.000000e+00 4.339261e-09]]\n",
            "SAR Model Prediction: [[1.501649e-28 1.000000e+00]]\n",
            "Averaged Prediction: [[0.33334446 0.66665554]]\n",
            "Probability of No Flooding: 33.33%\n",
            "Probability of Flooding: 66.67%\n",
            "Flooding is likely with a 66.67% probability.\n",
            "Image downloaded as satellite_image.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f8991d83390>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/arshlaan/projext/codathon/fld/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
            "    def _clean_thread_parent_frames(\n",
            "\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing image file: /home/arshlaan/projext/codathon/satellite_image.png\n",
            "Converted non-TIFF image to 12 channels: (1118, 1024, 12)\n",
            "Shape after processing: (1118, 1024, 12)\n",
            "Shape after resizing: (256, 256, 12)\n",
            "Final tensor shape: (1, 256, 256, 12)\n",
            "RGB input shape: (1, 256, 256, 3)\n",
            "S2 input shape: (1, 256, 256, 12)\n",
            "SAR input shape: (1, 256, 256, 2)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "RGB Model Prediction: [[0.85231286 0.14768717]]\n",
            "S2 Model Prediction: [[0.9337778  0.06622223]]\n",
            "SAR Model Prediction: [[3.0293226e-08 1.0000000e+00]]\n",
            "Averaged Prediction: [[0.59536356 0.40463647]]\n",
            "Probability of No Flooding: 59.54%\n",
            "Probability of Flooding: 40.46%\n",
            "No flooding is detected with a 59.54% probability.\n",
            "Image downloaded as satellite_image.png\n",
            "Processing image file: /home/arshlaan/projext/codathon/satellite_image.png\n",
            "Converted non-TIFF image to 12 channels: (1126, 887, 12)\n",
            "Shape after processing: (1126, 887, 12)\n",
            "Shape after resizing: (256, 256, 12)\n",
            "Final tensor shape: (1, 256, 256, 12)\n",
            "RGB input shape: (1, 256, 256, 3)\n",
            "S2 input shape: (1, 256, 256, 12)\n",
            "SAR input shape: (1, 256, 256, 2)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Run predictions from each model\u001b[39;00m\n\u001b[32m    123\u001b[39m pred_rgb = model_rgb.predict(tensor_image_rgb)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m pred_s2  = \u001b[43mmodel_s2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_image_s2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m pred_sar = model_sar.predict(tensor_image_sar)\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Average the predictions for an ensemble output\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:560\u001b[39m, in \u001b[36mTensorFlowTrainer.predict\u001b[39m\u001b[34m(self, x, batch_size, verbose, steps, callbacks)\u001b[39m\n\u001b[32m    558\u001b[39m callbacks.on_predict_batch_begin(step)\n\u001b[32m    559\u001b[39m data = get_data(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m batch_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m outputs = append_to_outputs(batch_outputs, outputs)\n\u001b[32m    562\u001b[39m callbacks.on_predict_batch_end(step, {\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: batch_outputs})\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:889\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    887\u001b[39m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[32m    888\u001b[39m   initializers = []\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    891\u001b[39m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[32m    892\u001b[39m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[32m    893\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:696\u001b[39m, in \u001b[36mFunction._initialize\u001b[39m\u001b[34m(self, args, kwds, add_initializers_to)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28mself\u001b[39m._variable_creation_config = \u001b[38;5;28mself\u001b[39m._generate_scoped_tracing_options(\n\u001b[32m    692\u001b[39m     variable_capturing_scope,\n\u001b[32m    693\u001b[39m     tracing_compilation.ScopeType.VARIABLE_CREATION,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    695\u001b[39m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvalid_creator_scope\u001b[39m(*unused_args, **unused_kwds):\n\u001b[32m    701\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[39m, in \u001b[36mtrace_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    175\u001b[39m     args = tracing_options.input_signature\n\u001b[32m    176\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m   concrete_function = \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options.bind_graph_to_function:\n\u001b[32m    183\u001b[39m   concrete_function._garbage_collector.release()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[39m, in \u001b[36m_maybe_define_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    282\u001b[39m   target_func_type = lookup_func_type\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m concrete_function = \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m   tracing_options.function_cache.add(\n\u001b[32m    289\u001b[39m       concrete_function, current_func_context\n\u001b[32m    290\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[39m, in \u001b[36m_create_concrete_function\u001b[39m\u001b[34m(function_type, type_context, func_graph, tracing_options)\u001b[39m\n\u001b[32m    303\u001b[39m   placeholder_bound_args = function_type.placeholder_arguments(\n\u001b[32m    304\u001b[39m       placeholder_context\n\u001b[32m    305\u001b[39m   )\n\u001b[32m    307\u001b[39m disable_acd = tracing_options.attributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options.attributes.get(\n\u001b[32m    308\u001b[39m     attributes_lib.DISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    309\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m traced_func_graph = \u001b[43mfunc_graph_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_type_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m transform.apply_func_graph_transforms(traced_func_graph)\n\u001b[32m    324\u001b[39m graph_capture_container = traced_func_graph.function_captures\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:1060\u001b[39m, in \u001b[36mfunc_graph_from_py_func\u001b[39m\u001b[34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[39m\n\u001b[32m   1057\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m   1059\u001b[39m _, original_func = tf_decorator.unwrap(python_func)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m func_outputs = \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[32m   1064\u001b[39m func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:599\u001b[39m, in \u001b[36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m default_graph._variable_creator_scope(scope, priority=\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    596\u001b[39m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     out = \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[39m, in \u001b[36mpy_func_from_autograph.<locals>.autograph_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m     51\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mag_error_metadata\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:339\u001b[39m, in \u001b[36mconverted_call\u001b[39m\u001b[34m(f, args, kwargs, caller_fn_scope, options)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[32m    338\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: AutoGraph artifact\u001b[39m\u001b[33m'\u001b[39m, f)\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools.partial):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[39m, in \u001b[36m_call_unconverted\u001b[39m\u001b[34m(f, args, kwargs, options, update_cache)\u001b[39m\n\u001b[32m    456\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f.\u001b[34m__self__\u001b[39m.call(args, kwargs)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:259\u001b[39m, in \u001b[36mTensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mone_step_on_data_distributed\u001b[39m(data):\n\u001b[32m    258\u001b[39m     data = data[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mone_step_on_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m     outputs = reduce_per_replica(\n\u001b[32m    263\u001b[39m         outputs,\n\u001b[32m    264\u001b[39m         \u001b[38;5;28mself\u001b[39m.distribute_strategy,\n\u001b[32m    265\u001b[39m         reduction=\u001b[33m\"\u001b[39m\u001b[33mconcat\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    266\u001b[39m     )\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py:1673\u001b[39m, in \u001b[36mStrategyBase.run\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1668\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scope():\n\u001b[32m   1669\u001b[39m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[32m   1670\u001b[39m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[32m   1671\u001b[39m   fn = autograph.tf_convert(\n\u001b[32m   1672\u001b[39m       fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extended\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py:3263\u001b[39m, in \u001b[36mStrategyExtendedV1.call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   3261\u001b[39m   kwargs = {}\n\u001b[32m   3262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py:4061\u001b[39m, in \u001b[36m_DefaultDistributionExtended._call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   4059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[32m   4060\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m._container_strategy(), replica_id_in_sync_group=\u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4061\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:249\u001b[39m, in \u001b[36mTensorFlowTrainer.make_predict_function.<locals>.one_step_on_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mone_step_on_data\u001b[39m(data):\n\u001b[32m    248\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs a predict test step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:104\u001b[39m, in \u001b[36mTensorFlowTrainer.predict_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    102\u001b[39m x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_has_training_arg:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    106\u001b[39m     y_pred = \u001b[38;5;28mself\u001b[39m(x)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/layers/layer.py:909\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    907\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    912\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    913\u001b[39m distribution = distribution_lib.distribution()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/ops/operation.py:52\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     48\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     49\u001b[39m         call_fn,\n\u001b[32m     50\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     51\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/models/functional.py:183\u001b[39m, in \u001b[36mFunctional.call\u001b[39m\u001b[34m(self, inputs, training, mask)\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    182\u001b[39m             backend.set_keras_mask(x, mask)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/ops/function.py:171\u001b[39m, in \u001b[36mFunction._run_through_graph\u001b[39m\u001b[34m(self, inputs, operation_fn, call_fn)\u001b[39m\n\u001b[32m    169\u001b[39m     outputs = call_fn(op, *args, **kwargs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     outputs = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node.outputs, tree.flatten(outputs)):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/models/functional.py:643\u001b[39m, in \u001b[36moperation_fn.<locals>.call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mhasattr\u001b[39m(operation, \u001b[33m\"\u001b[39m\u001b[33m_call_has_training_arg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    639\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m operation._call_has_training_arg\n\u001b[32m    640\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    641\u001b[39m ):\n\u001b[32m    642\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m] = training\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/layers/layer.py:909\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    907\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    912\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    913\u001b[39m distribution = distribution_lib.distribution()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/ops/operation.py:52\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     48\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     49\u001b[39m         call_fn,\n\u001b[32m     50\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     51\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/layers/normalization/batch_normalization.py:251\u001b[39m, in \u001b[36mBatchNormalization.call\u001b[39m\u001b[34m(self, inputs, training, mask)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# BN is prone to overflow with float16/bfloat16 inputs, so we upcast to\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# float32 for the subsequent computations.\u001b[39;00m\n\u001b[32m    249\u001b[39m inputs = ops.cast(inputs, compute_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m moving_mean = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmoving_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m moving_variance = ops.cast(\u001b[38;5;28mself\u001b[39m.moving_variance, inputs.dtype)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainable:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/ops/core.py:803\u001b[39m, in \u001b[36mcast\u001b[39m\u001b[34m(x, dtype)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Cast(dtype=dtype)(x)\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:204\u001b[39m, in \u001b[36mcast\u001b[39m\u001b[34m(x, dtype)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py:1012\u001b[39m, in \u001b[36mcast\u001b[39m\u001b[34m(x, dtype, name)\u001b[39m\n\u001b[32m   1006\u001b[39m   x = indexed_slices.IndexedSlices(values_cast, x.indices, x.dense_shape)\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1008\u001b[39m   \u001b[38;5;66;03m# TODO(josh11b): If x is not already a Tensor, we could return\u001b[39;00m\n\u001b[32m   1009\u001b[39m   \u001b[38;5;66;03m# ops.convert_to_tensor(x, dtype=dtype, ...)  here, but that\u001b[39;00m\n\u001b[32m   1010\u001b[39m   \u001b[38;5;66;03m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[39;00m\n\u001b[32m   1011\u001b[39m   \u001b[38;5;66;03m# strings.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m   x = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m x.dtype.is_complex \u001b[38;5;129;01mand\u001b[39;00m base_type.is_floating:\n\u001b[32m   1014\u001b[39m     logging.warn(\n\u001b[32m   1015\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are casting an input of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1016\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mincompatible dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_type.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.  This will \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdiscard the imaginary part and may not be what you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mintended.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[39m, in \u001b[36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, **trace_kwargs):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:736\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[32m    735\u001b[39m preferred_dtype = preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:209\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    207\u001b[39m overload = \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[33m\"\u001b[39m\u001b[33m__tf_tensor__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[32m    212\u001b[39m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[32m    213\u001b[39m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[32m    214\u001b[39m   ret = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:80\u001b[39m, in \u001b[36mVariable.__tf_tensor__\u001b[39m\u001b[34m(self, dtype, name)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:161\u001b[39m, in \u001b[36mconvert_to_tensor_v2_with_dispatch\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m.tf_export(\u001b[33m\"\u001b[39m\u001b[33mconvert_to_tensor\u001b[39m\u001b[33m\"\u001b[39m, v1=[])\n\u001b[32m     97\u001b[39m \u001b[38;5;129m@dispatch\u001b[39m.add_dispatch_support\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[32m     99\u001b[39m     value, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    100\u001b[39m ) -> tensor_lib.Tensor:\n\u001b[32m    101\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[32m    102\u001b[39m \n\u001b[32m    103\u001b[39m \u001b[33;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:171\u001b[39m, in \u001b[36mconvert_to_tensor_v2\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    225\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    226\u001b[39m           _add_error_prefix(\n\u001b[32m    227\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret.dtype.base_dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    231\u001b[39m               name=name))\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m   ret = \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    237\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:2378\u001b[39m, in \u001b[36m_dense_var_to_tensor\u001b[39m\u001b[34m(var, dtype, name, as_ref)\u001b[39m\n\u001b[32m   2377\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_dense_var_to_tensor\u001b[39m(var, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m, as_ref=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2378\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvar\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dense_var_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:1624\u001b[39m, in \u001b[36mBaseResourceVariable._dense_var_to_tensor\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1622\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_value().op.inputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:658\u001b[39m, in \u001b[36mBaseResourceVariable.value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    656\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_value\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:843\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op\u001b[39m\u001b[34m(self, no_copy)\u001b[39m\n\u001b[32m    841\u001b[39m       result = read_and_set_handle(no_copy)\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m   result = \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context.executing_eagerly():\n\u001b[32m    846\u001b[39m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[32m    847\u001b[39m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[32m    848\u001b[39m   record.record_operation(\n\u001b[32m    849\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mReadVariableOp\u001b[39m\u001b[33m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m.handle],\n\u001b[32m    850\u001b[39m       backward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[32m    851\u001b[39m       forward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:833\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[39m\u001b[34m(no_copy)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat.forward_compatible(\u001b[32m2022\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m3\u001b[39m):\n\u001b[32m    832\u001b[39m   gen_resource_variable_ops.disable_copy_on_read(\u001b[38;5;28mself\u001b[39m.handle)\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m result = \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m._dtype, \u001b[38;5;28mself\u001b[39m.handle, result)\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py:548\u001b[39m, in \u001b[36mread_variable_op\u001b[39m\u001b[34m(resource, dtype, name)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[32m    547\u001b[39m dtype = _execute.make_type(dtype, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m _, _, _op, _outputs = \u001b[43m_op_def_library\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReadVariableOp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m _result = _outputs[:]\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _execute.must_record_gradient():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/op_def_library.py:778\u001b[39m, in \u001b[36m_apply_op_helper\u001b[39m\u001b[34m(op_type_name, name, **keywords)\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m g.as_default(), ops.name_scope(name) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[32m    777\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m fallback:\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     \u001b[43m_ExtractInputsAndAttrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_list_attr_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_type_attr_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                           \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[32m    782\u001b[39m                            default_type_attr_map, attrs)\n\u001b[32m    783\u001b[39m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/op_def_library.py:551\u001b[39m, in \u001b[36m_ExtractInputsAndAttrs\u001b[39m\u001b[34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[39m\n\u001b[32m    545\u001b[39m       values = ops.convert_to_tensor(\n\u001b[32m    546\u001b[39m           values,\n\u001b[32m    547\u001b[39m           name=input_arg.name,\n\u001b[32m    548\u001b[39m           as_ref=input_arg.is_ref,\n\u001b[32m    549\u001b[39m           preferred_dtype=default_dtype)\n\u001b[32m    550\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m     values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_arg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_arg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    558\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[39m, in \u001b[36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, **trace_kwargs):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:736\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[32m    735\u001b[39m preferred_dtype = preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:209\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    207\u001b[39m overload = \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[33m\"\u001b[39m\u001b[33m__tf_tensor__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[32m    212\u001b[39m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[32m    213\u001b[39m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[32m    214\u001b[39m   ret = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:630\u001b[39m, in \u001b[36m_EagerTensorBase.__tf_tensor__\u001b[39m\u001b[34m(self, dtype, name)\u001b[39m\n\u001b[32m    624\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m graph.building_function:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    626\u001b[39m         _add_error_prefix(\n\u001b[32m    627\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAttempting to capture an EagerTensor without \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    628\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbuilding a function.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    629\u001b[39m             name=name))\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcapture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().__tf_tensor__(dtype, name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:619\u001b[39m, in \u001b[36mFuncGraph.capture\u001b[39m\u001b[34m(self, tensor, name, shape)\u001b[39m\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcapture\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, name=\u001b[38;5;28;01mNone\u001b[39;00m, shape=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_captures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/core/function/capture/capture_container.py:141\u001b[39m, in \u001b[36mFunctionCaptures.capture_by_value\u001b[39m\u001b[34m(self, graph, tensor, name)\u001b[39m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m graph_const\n\u001b[32m    140\u001b[39m   \u001b[38;5;66;03m# Large EagerTensors and resources are captured with Placeholder ops\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_placeholder_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor.graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m graph:\n\u001b[32m    144\u001b[39m   graph._validate_in_scope(tensor)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/core/function/capture/capture_container.py:285\u001b[39m, in \u001b[36mFunctionCaptures._create_placeholder_helper\u001b[39m\u001b[34m(self, graph, tensor, name)\u001b[39m\n\u001b[32m    279\u001b[39m   composite_device_name = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    280\u001b[39m placeholder_ctx = trace_type.InternalPlaceholderContext(\n\u001b[32m    281\u001b[39m     graph,\n\u001b[32m    282\u001b[39m     with_none_control_dependencies=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    283\u001b[39m     composite_device_name=composite_device_name,\n\u001b[32m    284\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m placeholder = \u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplaceholder_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplaceholder_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[38;5;28mself\u001b[39m.add_or_replace(\n\u001b[32m    287\u001b[39m     key=\u001b[38;5;28mid\u001b[39m(tensor), external=tensor, internal=placeholder, is_by_ref=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    288\u001b[39m )\n\u001b[32m    289\u001b[39m graph.inputs.append(placeholder)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/tensor.py:1019\u001b[39m, in \u001b[36mTensorSpec.placeholder_value\u001b[39m\u001b[34m(self, placeholder_context)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m placeholder_context.with_none_control_dependencies:\n\u001b[32m   1016\u001b[39m   \u001b[38;5;66;03m# Note: setting ops.control_dependencies(None) ensures we always put\u001b[39;00m\n\u001b[32m   1017\u001b[39m   \u001b[38;5;66;03m# capturing placeholders outside of any control flow context.\u001b[39;00m\n\u001b[32m   1018\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m context_graph.control_dependencies(\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     placeholder = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph_placeholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1021\u001b[39m   placeholder = \u001b[38;5;28mself\u001b[39m._graph_placeholder(context_graph, name=name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/tensor.py:1059\u001b[39m, in \u001b[36mTensorSpec._graph_placeholder\u001b[39m\u001b[34m(self, graph, name)\u001b[39m\n\u001b[32m   1057\u001b[39m attrs = {\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: dtype_value, \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: shape}\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m   op = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m   1060\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPlaceholder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1063\u001b[39m   \u001b[38;5;66;03m# TODO(b/262413656) Sometimes parameter names are not valid op names, in\u001b[39;00m\n\u001b[32m   1064\u001b[39m   \u001b[38;5;66;03m# which case an unnamed placeholder is created instead. Update this logic\u001b[39;00m\n\u001b[32m   1065\u001b[39m   \u001b[38;5;66;03m# to sanitize the name instead of falling back on unnamed placeholders.\u001b[39;00m\n\u001b[32m   1066\u001b[39m   logging.warning(e)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:614\u001b[39m, in \u001b[36mFuncGraph._create_op_internal\u001b[39m\u001b[34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[39m\n\u001b[32m    612\u001b[39m   inp = \u001b[38;5;28mself\u001b[39m.capture(inp)\n\u001b[32m    613\u001b[39m   captured_inputs.append(inp)\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:2698\u001b[39m, in \u001b[36mGraph._create_op_internal\u001b[39m\u001b[34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[39m\n\u001b[32m   2695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2696\u001b[39m   name = \u001b[38;5;28mself\u001b[39m.unique_name(name)\n\u001b[32m-> \u001b[39m\u001b[32m2698\u001b[39m node_def = \u001b[43m_NodeDef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2700\u001b[39m input_ops = \u001b[38;5;28mset\u001b[39m(t.op \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[32m   2701\u001b[39m control_inputs = \u001b[38;5;28mself\u001b[39m._control_dependencies_for_inputs(input_ops)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:984\u001b[39m, in \u001b[36m_NodeDef\u001b[39m\u001b[34m(op_type, name, attrs)\u001b[39m\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_NodeDef\u001b[39m(op_type, name, attrs=\u001b[38;5;28;01mNone\u001b[39;00m) -> node_def_pb2.NodeDef:\n\u001b[32m    971\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Create a NodeDef proto.\u001b[39;00m\n\u001b[32m    972\u001b[39m \n\u001b[32m    973\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    981\u001b[39m \u001b[33;03m    A node_def_pb2.NodeDef protocol buffer.\u001b[39;00m\n\u001b[32m    982\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m    983\u001b[39m   node_def = node_def_pb2.NodeDef(op=compat.as_bytes(op_type),\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m                                   name=\u001b[43mcompat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    985\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m attrs:\n\u001b[32m    986\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m attrs.items():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projext/codathon/fld/lib/python3.11/site-packages/tensorflow/python/util/compat.py:77\u001b[39m, in \u001b[36mas_bytes\u001b[39m\u001b[34m(bytes_or_text, encoding)\u001b[39m\n\u001b[32m     75\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(bytes_or_text)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bytes_or_text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m bytes_or_text.encode(encoding)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bytes_or_text, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m     79\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m bytes_or_text\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sentinelhub import SHConfig, BBox, CRS, SentinelHubRequest, DataCollection, MimeType\n",
        "from sentinelhub import SentinelHubRequest, MimeType, CRS, BBox, bbox_to_dimensions, DataCollection\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/home/arshlaan/projext/codathon/flood-risk-in-india/flood_risk_dataset_india.csv\")  # Update with the correct file name\n",
        "\n",
        "# Sort the dataset by flood probability in descending order\n",
        "df_sorted = df.sort_values(by=\"Flood Occurred\", ascending=False)\n",
        "\n",
        "\n",
        "    \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import rasterio\n",
        "\n",
        "\n",
        "for index, row in df_sorted.iterrows():\n",
        "    latitude = row[\"Latitude\"]\n",
        "    longitude = row[\"Longitude\"]\n",
        "\n",
        "    bbox = BBox([longitude - 0.01, latitude - 0.01, longitude + 0.01, latitude + 0.01], CRS.WGS84)\n",
        "\n",
        "    # Define image resolution\n",
        "    resolution = 2   # 10m per pixel\n",
        "\n",
        "    # Get dimensions\n",
        "    size = bbox_to_dimensions(bbox, resolution=resolution)\n",
        "\n",
        "    # Create request for Sentinel-2 image\n",
        "    request = SentinelHubRequest(\n",
        "        evalscript=\"\"\"\n",
        "        // Extract RGB image\n",
        "        function setup() {\n",
        "            return {\n",
        "                input: [\"B04\", \"B03\", \"B02\"],\n",
        "                output: { bands: 3 }\n",
        "            };\n",
        "        }\n",
        "        function evaluatePixel(sample) {\n",
        "            return [sample.B04, sample.B03, sample.B02];\n",
        "        }\n",
        "        \"\"\",\n",
        "        input_data=[SentinelHubRequest.input_data(DataCollection.SENTINEL2_L2A, maxcc=0.1, time_interval=(\"2024-01-01\", \"2024-12-31\"))],  # Fixed maxcc\n",
        "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n",
        "        bbox=bbox,\n",
        "        size=size\n",
        "    )\n",
        "\n",
        "    # Get image\n",
        "    image = request.get_data()[0]\n",
        "\n",
        "    # Save image\n",
        "    import imageio\n",
        "    imageio.imwrite(\"satellite_image.png\", image)\n",
        "\n",
        "    print(\"Image downloaded as satellite_image.png\")\n",
        "        \n",
        "    # Load models (using compile=False to avoid optimizer warnings)\n",
        "    model_rgb = tf.keras.models.load_model(\"/home/arshlaan/projext/codathon/RGB_CNN.h5\", compile=False)\n",
        "    model_s2  = tf.keras.models.load_model(\"/home/arshlaan/projext/codathon/S2_CNN.h5\", compile=False)\n",
        "    model_sar = tf.keras.models.load_model(\"/home/arshlaan/projext/codathon/SAR_CNN.h5\", compile=False)\n",
        "\n",
        "    # Load the image file\n",
        "    image_path = \"/home/arshlaan/projext/codathon/satellite_image.png\"  # Replace with the actual path to your image\n",
        "    print(\"Processing image file:\", image_path)\n",
        "\n",
        "    # Load the image using rasterio if it's a .tif\n",
        "    # if image_path.lower().endswith('.tiff'):\n",
        "        # with rasterio.open(image_path) as src:\n",
        "        #     image_array = src.read()  # shape: (channels, height, width)\n",
        "        # print(\"Original shape from rasterio:\", image_array.shape)\n",
        "\n",
        "        # # Handle channels for S2 model:\n",
        "        # if image_array.shape[0] == 1:\n",
        "        #     image_array = np.repeat(image_array, 12, axis=0)\n",
        "        #     print(\"Duplicated single channel to 12 channels:\", image_array.shape)\n",
        "        # elif image_array.shape[0] < 12:\n",
        "        #     padding_channels = 12 - image_array.shape[0]\n",
        "        #     image_array = np.pad(image_array, ((0, padding_channels), (0,0), (0,0)), mode='constant')\n",
        "        #     print(\"Padded to 12 channels:\", image_array.shape)\n",
        "        # elif image_array.shape[0] > 12:\n",
        "        #     image_array = image_array[:12, :, :]\n",
        "        #     print(\"Using the first 12 channels:\", image_array.shape)\n",
        "\n",
        "        # # Convert from (channels, height, width) to (height, width, channels)\n",
        "        # image_array = np.moveaxis(image_array, 0, -1)\n",
        "    # if(true):\n",
        "        # For non-TIFF images, load with PIL and convert to RGB, then pad to 12 channels if needed\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_array = np.array(image)\n",
        "    if image_array.shape[-1] == 3:\n",
        "        padding_channels = 12 - 3\n",
        "        image_array = np.concatenate([image_array, np.zeros((image_array.shape[0], image_array.shape[1], padding_channels), dtype=image_array.dtype)], axis=-1)\n",
        "        print(\"Converted non-TIFF image to 12 channels:\", image_array.shape)\n",
        "\n",
        "    print(\"Shape after processing:\", image_array.shape)\n",
        "\n",
        "    # Resize the image to 256x256 (model's expected input size)\n",
        "    image_resized = cv2.resize(image_array, (256, 256))\n",
        "    print(\"Shape after resizing:\", image_resized.shape)\n",
        "\n",
        "    # Normalize the image (assuming pixel values range 0-255)\n",
        "    image_normalized = image_resized / 255.0\n",
        "\n",
        "    # Convert to tensor and add a batch dimension: expected shape (1, 256, 256, 12)\n",
        "    tensor_image = tf.convert_to_tensor(image_normalized, dtype=tf.float32)\n",
        "    tensor_image = tf.expand_dims(tensor_image, axis=0)\n",
        "    print(\"Final tensor shape:\", tensor_image.shape)\n",
        "\n",
        "    # Prepare inputs for each model:\n",
        "    tensor_image_rgb = tensor_image[..., :3]  # First 3 channels for RGB model\n",
        "    tensor_image_s2 = tensor_image  # All 12 channels for S2 model\n",
        "    tensor_image_sar = tensor_image[..., :2]  # First 2 channels for SAR model\n",
        "\n",
        "    print(\"RGB input shape:\", tensor_image_rgb.shape)\n",
        "    print(\"S2 input shape:\", tensor_image_s2.shape)\n",
        "    print(\"SAR input shape:\", tensor_image_sar.shape)\n",
        "\n",
        "    # Run predictions from each model\n",
        "    pred_rgb = model_rgb.predict(tensor_image_rgb)\n",
        "    pred_s2  = model_s2.predict(tensor_image_s2)\n",
        "    pred_sar = model_sar.predict(tensor_image_sar)\n",
        "\n",
        "    # Average the predictions for an ensemble output\n",
        "    avg_pred = np.mean([pred_rgb, pred_s2, pred_sar], axis=0)\n",
        "\n",
        "    print(\"RGB Model Prediction:\", pred_rgb)\n",
        "    print(\"S2 Model Prediction:\", pred_s2)\n",
        "    print(\"SAR Model Prediction:\", pred_sar)\n",
        "    print(\"Averaged Prediction:\", avg_pred)\n",
        "\n",
        "    # Assuming avg_pred is [[prob_no_flood, prob_flood]]\n",
        "    prob_no_flood = avg_pred[0][0] * 100\n",
        "    prob_flood = avg_pred[0][1] * 100\n",
        "\n",
        "    # Print predictions in percentage\n",
        "    print(f\"Probability of No Flooding: {prob_no_flood:.2f}%\")\n",
        "    print(f\"Probability of Flooding: {prob_flood:.2f}%\")\n",
        "\n",
        "    # Determine and print a message based on which probability is higher\n",
        "    if prob_flood > prob_no_flood:\n",
        "        print(f\"Flooding is likely with a {prob_flood:.2f}% probability.\")\n",
        "    else:\n",
        "        print(f\"No flooding is detected with a {prob_no_flood:.2f}% probability.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 2227758,
          "sourceId": 3725387,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30197,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "fld",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
